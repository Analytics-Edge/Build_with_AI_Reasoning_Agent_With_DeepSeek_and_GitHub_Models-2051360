{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "250258a4-13bd-4043-b568-0a5dbdda4d7b",
   "metadata": {},
   "source": [
    "A/B testing is your secret weapon for making data-driven decisions about AI model performance. Instead of guessing whether your DeepSeek reasoning agent is better than alternatives, you'll gather real evidence from actual usage patterns. The best part? You can implement this with simple Python scripts—no complex infrastructure required\\!\n",
    "\n",
    "Think of A/B testing as a controlled scientific experiment for your AI systems. You'll split your traffic between two approaches: the 'control group' using your current model and the 'treatment group' using your new DeepSeek reasoning agent. By comparing their performance with real users and real queries, you get definitive answers about which approach works better.\n",
    "\n",
    "**Understanding What You'll Build**\n",
    "\n",
    "In this chapter, you will create a traffic routing system that:\n",
    "\n",
    "·       Sends 95% of requests to your baseline model (control group)\n",
    "\n",
    "·       Sends 5% of requests to your DeepSeek reasoning agent (treatment group)\n",
    "\n",
    "·       Consistently assigns the same users to the same group\n",
    "\n",
    "·       Collects comprehensive metrics for analysis\n",
    "\n",
    "·       Provides clear, actionable performance comparisons\n",
    "\n",
    "**Setting Up Your A/B Testing Environment**\n",
    "\n",
    "**Step 1: Create Your A/B Test Router File**\n",
    "\n",
    "First, you'll create a new Python file to house your A/B testing system.\n",
    "\n",
    "1\\.   \tIn your file explorer (left panel of your development environment), right-click in an empty area\n",
    "\n",
    "2\\.   \tSelect \"New File\" from the context menu that appears\n",
    "\n",
    "3\\.   \tType ab\\_test\\_router.py as the filename and press Enter\n",
    "\n",
    "You should now see a new, empty file open in your code editor in the center panel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fff9a45-db02-4f86-8bed-cd413ec28fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65d372d0-6089-400f-93fa-75c7e5cdd6f9",
   "metadata": {},
   "source": [
    "Code editor showing A/B testing router implementation with syntax highlighting\n",
    "\n",
    "**Step 2: Implement the Core A/B Testing Router**\n",
    "\n",
    "Now you'll write the traffic splitting mechanism. Type the following code into your ab\\_test\\_router.py file, paying careful attention to indentation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9978c56-c76e-4025-814d-019989c4367b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random  \n",
    " import hashlib  \n",
    " import time\n",
    "\n",
    " class ABTestRouter:  \n",
    " \tdef \\_\\_init\\_\\_(self):  \n",
    "     \tself.treatment\\_percentage \\= 0.05  \\# 5% to reasoning agent  \n",
    "     \tself.results \\= \\[\\]  \n",
    "     \t  \n",
    " \tdef get\\_user\\_group(self, user\\_id):  \n",
    "     \t\\# Consistent assignment based on user ID hash  \n",
    "     \thash\\_value \\= int(hashlib.md5(user\\_id.encode()).hexdigest()\\[:8\\], 16\\)  \n",
    "     \tnormalized \\= hash\\_value / 0xFFFFFFFF  \n",
    "     \t  \n",
    "     \treturn \"treatment\" if normalized \\< self.treatment\\_percentage else \"control\"  \n",
    " \t  \n",
    " \tdef call\\_baseline\\_model(self, query):  \n",
    "     \t\\# Placeholder for your baseline model \\- replace with actual implementation  \n",
    "     \ttime.sleep(0.5)  \\# Simulate processing time  \n",
    "     \treturn f\"Baseline model response: {query\\[:30\\]}...\"  \n",
    " \t  \n",
    " \tdef call\\_reasoning\\_agent(self, query):  \n",
    "     \t\\# Placeholder for your DeepSeek agent \\- replace with actual implementation  \n",
    "     \ttime.sleep(1.2)  \\# DeepSeek typically takes longer due to reasoning  \n",
    "     \treturn f\"DeepSeek reasoning response: {query\\[:30\\]}... \\[with step-by-step reasoning\\]\"  \n",
    "     \t  \n",
    " \tdef route\\_request(self, user\\_id, query):  \n",
    "     \tgroup \\= self.get\\_user\\_group(user\\_id)  \n",
    "     \tstart\\_time \\= time.time()  \n",
    "     \t  \n",
    "     \tif group \\== \"treatment\":  \n",
    "         \tresponse \\= self.call\\_reasoning\\_agent(query)  \n",
    "     \telse:  \n",
    "         \tresponse \\= self.call\\_baseline\\_model(query)  \n",
    "     \t  \n",
    "     \tresponse\\_time \\= time.time() \\- start\\_time  \n",
    "     \t  \n",
    "     \t\\# Log the result for analysis  \n",
    "     \tself.results.append({  \n",
    "         \t\"user\\_id\": user\\_id,  \n",
    "         \t\"group\": group,  \n",
    "    \t     \"query\": query,  \n",
    "         \t\"response\": response,  \n",
    "         \t\"response\\_time\": response\\_time,  \n",
    "         \t\"timestamp\": time.time()  \n",
    "     \t})  \n",
    "     \t  \n",
    "     \treturn response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9401d636-581f-4814-9ec7-b8d47f484d5b",
   "metadata": {},
   "source": [
    "**What you should see on your screen:**\n",
    "\n",
    "·       The code editor displays your Python code with syntax highlighting\n",
    "\n",
    "·       Keywords like import, class, and def appear in different colors\n",
    "\n",
    "·       Indentation is clearly visible and consistent\n",
    "\n",
    "·       The file tab at the top shows ab\\_test\\_router.py with an unsaved indicator (usually a dot or asterisk)\n",
    "\n",
    "**Step 3: Save Your Router Implementation**\n",
    "\n",
    "1\\.   \tPress Ctrl+S (Windows/Linux) or Cmd+S (Mac) to save your file\n",
    "\n",
    "2\\.   \tYou should see the unsaved indicator disappear from the file tab\n",
    "\n",
    "3\\.   \tThe file explorer on the left should show ab\\_test\\_router.py without any indicators\n",
    "\n",
    "**Adding Comprehensive Metrics Analysis**\n",
    "\n",
    "Now you'll add powerful analysis capabilities to understand how your models compare. Add the following methods to your ABTestRouter class:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658987b3-0646-4cf4-9029-c18178421631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze\\_ab\\_results(self):  \n",
    " \tcontrol\\_results \\= \\[r for r in self.results if r\\[\"group\"\\] \\== \"control\"\\]  \n",
    " \ttreatment\\_results \\= \\[r for r in self.results if r\\[\"group\"\\] \\== \"treatment\"\\]  \n",
    " \t  \n",
    " \t\\# Check if we have enough data  \n",
    " \tif len(control\\_results) \\== 0 or len(treatment\\_results) \\== 0:  \n",
    "     \treturn {\"error\": \"Need data from both control and treatment groups\"}  \n",
    " \t  \n",
    " \t\\# Calculate key metrics  \n",
    " \tcontrol\\_avg\\_time \\= sum(r\\[\"response\\_time\"\\] for r in control\\_results) / len(control\\_results)  \n",
    " \ttreatment\\_avg\\_time \\= sum(r\\[\"response\\_time\"\\] for r in treatment\\_results) / len(treatment\\_results)  \n",
    " \t  \n",
    " \t\\# Quality scoring (implement based on your criteria)  \n",
    " \tcontrol\\_quality \\= sum(self.score\\_response(r\\[\"response\"\\]) for r in control\\_results) / len(control\\_results)  \n",
    " \ttreatment\\_quality \\= sum(self.score\\_response(r\\[\"response\"\\]) for r in treatment\\_results) / len(treatment\\_results)  \n",
    " \t  \n",
    " \treturn {  \n",
    "     \t\"control\\_samples\": len(control\\_results),  \n",
    "     \t\"treatment\\_samples\": len(treatment\\_results),  \n",
    "     \t\"control\\_avg\\_time\": control\\_avg\\_time,  \n",
    "     \t\"treatment\\_avg\\_time\": treatment\\_avg\\_time,  \n",
    "     \t\"time\\_improvement\": control\\_avg\\_time \\- treatment\\_avg\\_time,  \n",
    "     \t\"control\\_quality\": control\\_quality,  \n",
    "     \t\"treatment\\_quality\": treatment\\_quality,  \n",
    "     \t\"quality\\_improvement\": treatment\\_quality \\- control\\_quality  \n",
    " \t}\n",
    "\n",
    " def score\\_response(self, response):  \n",
    " \t\\# Simple quality scoring \\- enhance this based on your specific needs  \n",
    " \tscore \\= 0.5  \\# Base score  \n",
    " \t  \n",
    " \t\\# Higher score for longer, more detailed responses  \n",
    " \tif len(response) \\> 50:  \n",
    "     \tscore \\+= 0.2  \n",
    " \t  \n",
    " \t\\# Higher score for responses that mention reasoning  \n",
    " \tif \"reasoning\" in response.lower():  \n",
    "     \tscore \\+= 0.3  \n",
    "     \t  \n",
    " \treturn min(score, 1.0)  \\# Cap at 1.0\n",
    "\n",
    " def print\\_results\\_table(self):  \n",
    " \tresults \\= self.analyze\\_ab\\_results()  \n",
    " \t  \n",
    " \tif \"error\" in results:  \n",
    "     \tprint(f\"Error: {results\\['error'\\]}\")  \n",
    "     \treturn  \n",
    " \t  \n",
    " \tprint(\"\\\\n\" \\+ \"=\"\\*60)  \n",
    " \tprint(\"A/B TEST RESULTS SUMMARY\")  \n",
    " \tprint(\"=\"\\*60)  \n",
    " \tprint(f\"Control Group Samples: \t{results\\['control\\_samples'\\]:\\>8}\")  \n",
    " \tprint(f\"Treatment Group Samples:   {results\\['treatment\\_samples'\\]:\\>8}\")  \n",
    " \tprint(f\"\")  \n",
    " \tprint(f\"Average Response Times:\")  \n",
    " \tprint(f\"  Control:                 {results\\['control\\_avg\\_time'\\]:\\>8.3f}s\")  \n",
    " \tprint(f\"  Treatment:               {results\\['treatment\\_avg\\_time'\\]:\\>8.3f}s\")  \n",
    " \tprint(f\"  Improvement:             {results\\['time\\_improvement'\\]:\\>8.3f}s\")  \n",
    " \tprint(f\"\")  \n",
    " \tprint(f\"Quality Scores:\")  \n",
    " \tprint(f\"  Control:                 {results\\['control\\_quality'\\]:\\>8.3f}\")  \n",
    "     print(f\"  Treatment:               {results\\['treatment\\_quality'\\]:\\>8.3f}\")  \n",
    " \tprint(f\"  Improvement:             {results\\['quality\\_improvement'\\]:\\>8.3f}\")  \n",
    " \tprint(\"=\"\\*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01398160-b62c-409f-a322-2d2564b6278f",
   "metadata": {},
   "source": [
    "**What this code does:**\n",
    "\n",
    "·       **analyze\\_ab\\_results()**: Computes key performance metrics comparing control and treatment groups\n",
    "\n",
    "·       **score\\_response()**: Evaluates response quality based on length and reasoning content\n",
    "\n",
    "·       **print\\_results\\_table()**: Displays results in a clear, readable table format\n",
    "\n",
    "**Testing Your A/B Testing System**\n",
    "\n",
    "Now let's create a test script to see your A/B testing router in action. Create a new file called test\\_ab\\_router.py:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e2ba6d-76f0-4b2c-815e-92078c5329f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ab\\_test\\_router import ABTestRouter\n",
    "\n",
    " def run\\_ab\\_test\\_simulation():  \n",
    " \t\\# Create the router  \n",
    " \trouter \\= ABTestRouter()  \n",
    " \t  \n",
    " \t\\# Simulate different users and queries  \n",
    " \ttest\\_queries \\= \\[  \n",
    "     \t\"What is machine learning?\",  \n",
    "     \t\"How does artificial intelligence work?\",  \n",
    "     \t\"Explain neural networks in simple terms.\",  \n",
    "     \t\"What are the benefits of renewable energy?\",  \n",
    "     \t\"How do I improve my productivity?\",  \n",
    "     \t\"What causes climate change?\",  \n",
    "     \t\"Describe quantum computing.\",  \n",
    "     \t\"How can I learn programming?\",  \n",
    "     \t\"What is the future of AI?\",  \n",
    "     \t\"Explain blockchain technology.\"  \n",
    " \t\\]  \n",
    " \t  \n",
    " \t\\# Simulate 100 different users making requests  \n",
    " \tprint(\"Running A/B test simulation...\")  \n",
    " \tfor user\\_num in range(1, 101):  \n",
    "     \tuser\\_id \\= f\"user\\_{user\\_num:03d}\"  \n",
    "     \tquery \\= test\\_queries\\[user\\_num % len(test\\_queries)\\]  \n",
    "     \t  \n",
    "     \tresponse \\= router.route\\_request(user\\_id, query)  \n",
    "     \t  \n",
    "     \t\\# Show progress every 20 requests  \n",
    "     \tif user\\_num % 20 \\== 0:  \n",
    "         \tprint(f\"Processed {user\\_num} requests...\")  \n",
    " \t  \n",
    " \tprint(f\"Simulation complete\\! Processed {len(router.results)} total requests.\")  \n",
    " \t  \n",
    " \t\\# Display results  \n",
    " \trouter.print\\_results\\_table()\n",
    "\n",
    " if \\_\\_name\\_\\_ \\== \"\\_\\_main\\_\\_\":  \n",
    " \trun\\_ab\\_test\\_simulation()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e6b6cb-d4f1-4606-85b5-aff6c806f0a2",
   "metadata": {},
   "source": [
    "**Step 4: Run Your A/B Test Simulation**\n",
    "\n",
    "1\\.   \tOpen the terminal panel (bottom of your screen) by clicking on it or pressing \\`Ctrl+\\`\\` (backtick)\n",
    "\n",
    "2\\.   \tType python test\\_ab\\_router.py and press Enter\n",
    "\n",
    "**What you should see in the terminal:**\n",
    "\n",
    "·       Progress messages showing requests being processed\n",
    "\n",
    "·       A completion message\n",
    "\n",
    "·       A detailed results table similar to this:\n",
    "\n",
    "The table will show you:\n",
    "\n",
    "·       **Sample sizes**: How many requests went to each group (should be approximately 95/5 split)\n",
    "\n",
    "·       **Response times**: Average processing time for each group\n",
    "\n",
    "·       **Quality scores**: Comparative quality ratings\n",
    "\n",
    "·       **Improvements**: Direct comparison showing which model performs better\n",
    "\n",
    "**Understanding Your Results**\n",
    "\n",
    "**Interpreting the Metrics**\n",
    "\n",
    "**Sample Distribution:**\n",
    "\n",
    "·       Control group should have \\~95 samples\n",
    "\n",
    "·       Treatment group should have \\~5 samples\n",
    "\n",
    "·       This confirms your 5% traffic split is working correctly\n",
    "\n",
    "**Response Time Analysis:**\n",
    "\n",
    "·       Positive \"Improvement\" means the treatment (DeepSeek) is faster\n",
    "\n",
    "·       Negative \"Improvement\" means the treatment is slower\n",
    "\n",
    "·       Consider whether quality gains justify any speed differences\n",
    "\n",
    "**Quality Score Comparison:**\n",
    "\n",
    "·       Higher scores indicate better response quality\n",
    "\n",
    "·       Positive \"Improvement\" means DeepSeek provides better responses\n",
    "\n",
    "·       Use this to evaluate if reasoning capabilities add value\n",
    "\n",
    "**Making Decisions Based on Results**\n",
    "\n",
    "**If DeepSeek shows significant quality improvement:**\n",
    "\n",
    "·       Consider gradually increasing the traffic percentage (10%, 25%, 50%)\n",
    "\n",
    "·       Monitor costs and user satisfaction\n",
    "\n",
    "·       Document the improvement for stakeholders\n",
    "\n",
    "**If results are mixed or unclear:**\n",
    "\n",
    "·       Collect more data (aim for 1,000+ samples per group)\n",
    "\n",
    "·       Test with different types of queries\n",
    "\n",
    "·       Adjust quality scoring criteria to better reflect your needs\n",
    "\n",
    "**Best Practices for Production A/B Testing**\n",
    "\n",
    "**Data Collection Guidelines**\n",
    "\n",
    "**Statistical Significance:**\n",
    "\n",
    "·       Collect at least 1,000 samples per group before making major decisions\n",
    "\n",
    "·       Run tests for multiple days to capture different usage patterns\n",
    "\n",
    "·       Consider weekday vs. weekend differences\n",
    "\n",
    "**User Experience Consistency:**\n",
    "\n",
    "·       The hash-based assignment ensures users always get the same model\n",
    "\n",
    "·       This prevents confusion from inconsistent AI behavior\n",
    "\n",
    "·       Users don't know they're part of an experiment\n",
    "\n",
    "**Monitoring and Safety**\n",
    "\n",
    "**Cost Monitoring:**\n",
    "\n",
    "·       Track API usage and costs for both models\n",
    "\n",
    "·       Set alerts if spending exceeds expected budgets\n",
    "\n",
    "·       Consider implementing automatic test stopping\n",
    "\n",
    "**Quality Assurance:**\n",
    "\n",
    "·       Regularly review sample responses from both groups\n",
    "\n",
    "·       Watch for any degradation in either model\n",
    "\n",
    "·       Have rollback procedures ready if issues arise\n",
    "\n",
    "**Scaling Your A/B Testing**\n",
    "\n",
    "**Advanced Metrics**\n",
    "\n",
    "As your system matures, consider tracking:\n",
    "\n",
    "·       User satisfaction scores (if you have feedback mechanisms)\n",
    "\n",
    "·       Task completion rates\n",
    "\n",
    "·       Conversation length and engagement\n",
    "\n",
    "·       Error rates and edge case handling\n",
    "\n",
    "**Multiple Test Variants**\n",
    "\n",
    "You can extend this system to test:\n",
    "\n",
    "·       Different DeepSeek temperature settings\n",
    "\n",
    "·       Various prompt engineering approaches\n",
    "\n",
    "·       Multiple AI models simultaneously\n",
    "\n",
    "·       Different reasoning explanation formats\n",
    "\n",
    "**Summary**\n",
    "\n",
    "You've now built a complete A/B testing system that:\n",
    "\n",
    " ✅ **Routes traffic intelligently** with consistent user assignment  \n",
    " ✅ **Collects comprehensive metrics** on performance and quality  \n",
    " ✅ **Provides clear analysis** with actionable insights  \n",
    " ✅ **Scales easily** for production deployment  \n",
    " ✅ **Maintains user experience** without revealing the experiment\n",
    "\n",
    "This straightforward approach gives you real, actionable data about your reasoning agent's performance. The insights you gain will guide not just this decision, but your entire AI strategy going forward.\n",
    "\n",
    "Remember to run your A/B test long enough to capture different usage patterns—weekdays vs weekends, different user types, varying query complexity. With proper data collection and analysis, you'll have the confidence to make informed decisions about deploying your DeepSeek reasoning agent to all your users.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
