{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2846d8e-9261-4ca7-b0dc-98a66500c69c",
   "metadata": {},
   "source": [
    "Deploying powerful AI agents can use significant compute resources—and those costs can add up quickly\\! In this chapter, you'll learn step-by-step how to keep your DeepSeek reasoning agent cost-effective without sacrificing performance. By the end of this chapter, you'll have practical tools to monitor, optimize, and control your AI agent costs.\n",
    "\n",
    "**Understanding and Tracking Your Usage Metrics**\n",
    "\n",
    "Before you can optimize costs, you need to understand exactly what resources your AI agent is consuming. You'll track three key metrics: tokens processed, compute time, and API calls.\n",
    "\n",
    "Let's start by adding logging hooks to your code that will capture these metrics automatically. You'll want to wrap your agent calls with timing and usage tracking.\n",
    "\n",
    "Here's how to implement basic usage tracking:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59680d0-cb33-4d4b-a1a1-751d6c6b3632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    " \\# Start timing before your agent executes a task  \n",
    " start \\= time.time()  \n",
    " response \\= task.execute()  \n",
    " duration \\= time.time() \\- start\n",
    "\n",
    " \\# Capture token usage from the response  \n",
    " tokens \\= response.usage.total\\_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb74d83-5222-4ce6-a307-8548d18b51c9",
   "metadata": {},
   "source": [
    "When you run this code, you'll see that start captures the exact moment before your task begins executing. After the task completes, duration will contain the total time in seconds that the operation took. The tokens variable extracts the total number of tokens processed from the response object—this includes both input and output tokens.\n",
    "\n",
    "**Calculating Your Actual Costs**\n",
    "\n",
    "Now that you're tracking usage metrics, you need to convert these numbers into actual dollar amounts. Different AI services have different pricing structures, but most charge based on tokens processed and compute time used.\n",
    "\n",
    "Here's how to calculate costs using typical pricing rates. For this example, we'll use $0.002 per 1,000 tokens and $0.05 per compute minute:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1e55c4-5358-4d30-99e7-9561f1e22cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\\# Calculate token costs  \n",
    " cost\\_tokens \\= tokens / 1000 \\* 0.002\n",
    "\n",
    " \\# Calculate compute time costs (convert seconds to minutes)  \n",
    " cost\\_compute \\= duration / 60 \\* 0.05\n",
    "\n",
    " \\# Get your total cost for this request  \n",
    " total\\_cost \\= cost\\_tokens \\+ cost\\_compute\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674d9c21-d137-4252-b33a-7d149f281763",
   "metadata": {},
   "source": [
    "When you run this calculation, cost\\_tokens will show you how much you spent on token processing, cost\\_compute will show your compute time costs, and total\\_cost gives you the complete cost for that single request.\n",
    "\n",
    "To track costs over time, you'll want to log each interaction and build a running total:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360cac4f-019c-44da-96af-3c17b9c91636",
   "metadata": {},
   "outputs": [],
   "source": [
    "\\# Initialize a list to store all your costs  \n",
    " costs \\= \\[\\]\n",
    "\n",
    " \\# After each request, add the cost and display it  \n",
    " costs.append(total\\_cost)  \n",
    " print(f\"Request cost: ${total\\_cost:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b123e96-61d2-4124-b542-a0b93c77acf9",
   "metadata": {},
   "source": [
    "You'll see output like \"Request cost: $0.0156\" after each interaction. The costs list will grow with each request, giving you a complete history of your spending.\n",
    "\n",
    "**Implementing Smart Cost-Saving Strategies**\n",
    "\n",
    "Once you understand your costs, you can implement several strategies to reduce them without losing functionality. You'll explore three powerful approaches: smart routing, response caching, and batch processing.\n",
    "\n",
    "**Smart Routing: Choose the Right Model for Each Task**\n",
    "\n",
    "Smart routing means sending simple queries to less expensive models and reserving powerful (more expensive) models for complex reasoning tasks. You'll create logic that evaluates each query's complexity and routes it appropriately.\n",
    "\n",
    "Here's how to implement smart routing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8202b0-e821-4b76-9568-495f5b7c5955",
   "metadata": {},
   "outputs": [],
   "source": [
    "\\# Define your complexity threshold (you'll adjust this based on your needs)  \n",
    " threshold \\= 50  \\# This could be based on prompt length, keywords, etc.\n",
    "\n",
    " \\# Route queries based on complexity  \n",
    " if complexity \\< threshold:  \n",
    " \tuse\\_model \\= \"gpt-3.5-turbo\"  \\# Cheaper model for simple tasks  \n",
    " else:  \n",
    " \tuse\\_model \\= \"deepseek-reasoner\"  \\# More expensive model for complex reasoning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d547f7c3-a0b9-4537-bc8c-cc28b1a266cf",
   "metadata": {},
   "source": [
    "When your code runs, it will automatically evaluate each query. Simple questions like \"What time is it?\" might go to the cheaper model, while complex reasoning tasks will use your more powerful (and expensive) model.\n",
    "\n",
    "**Response Caching: Store and Reuse Answers**\n",
    "\n",
    "Response caching stores answers to queries you've seen before, so you don't have to process them again. This is especially valuable when users ask similar questions repeatedly.\n",
    "\n",
    "Here's a basic caching implementation using a dictionary:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef70389c-4cbb-4869-9aa1-f0ae150b1af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\\# Initialize your cache dictionary  \n",
    " cache \\= {}\n",
    "\n",
    " def get\\_response(prompt):  \n",
    " \t\\# Create a unique key for this prompt  \n",
    " \tkey \\= hash(prompt)  \n",
    " \t  \n",
    " \t\\# Check if we've seen this prompt before  \n",
    " \tif key in cache:  \n",
    "     \treturn cache\\[key\\]  \\# Return cached response instantly  \n",
    " \t  \n",
    " \t\\# If not cached, get a new response  \n",
    " \tresponse \\= agent.ask(prompt)  \n",
    " \t  \n",
    " \t\\# Store the response for future use  \n",
    " \tcache\\[key\\] \\= response  \n",
    " \treturn response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f23cce-c348-4d2c-a866-6b43849326b2",
   "metadata": {},
   "source": [
    "When you use this function, you'll notice that the first time you ask a question, there's a normal processing delay. But if you ask the same question again, you'll get an instant response from the cache, saving both time and money.\n",
    "\n",
    "**Batch Processing: Group Multiple Tasks**\n",
    "\n",
    "Batch processing combines multiple small requests into single larger calls, which is often more cost-effective than processing requests individually.\n",
    "\n",
    "You can implement batch processing by collecting multiple queries and sending them together:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21941c6-c8b1-419d-adcd-5feead024b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\\# Collect multiple queries  \n",
    " batch\\_queries \\= \\[  \n",
    " \t\"What is the weather today?\",  \n",
    " \t\"Explain machine learning basics\",  \n",
    " \t\"Calculate 15% tip on $45\"  \n",
    " \\]\n",
    "\n",
    " \\# Process them as a single batch request  \n",
    " batch\\_response \\= agent.ask\\_batch(batch\\_queries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e059bb-9ec9-45d3-9783-87f51e58cd36",
   "metadata": {},
   "source": [
    "This approach reduces the overhead costs associated with multiple individual API calls.\n",
    "\n",
    "**Measuring Your Return on Investment**\n",
    "\n",
    "Cost optimization isn't just about spending less—it's about getting the best value for your investment. You'll want to measure whether cost increases lead to meaningful improvements in accuracy or user satisfaction.\n",
    "\n",
    "To calculate return on investment, compare your cost changes to performance gains. For example, if upgrading to a better model increases your costs by 20% but improves accuracy by 10%, you need to determine whether that accuracy improvement translates to business value that justifies the additional expense.\n",
    "\n",
    "You might track metrics like:\n",
    "\n",
    "·       User satisfaction scores before and after optimization\n",
    "\n",
    "·       Task completion rates\n",
    "\n",
    "·       Error rates and correction costs\n",
    "\n",
    "·       Time saved through improved performance\n",
    "\n",
    "**Setting Up Cost Monitoring and Alerts**\n",
    "\n",
    "To prevent unexpected cost spikes, you'll set up automated monitoring that alerts you when spending approaches your budget limits.\n",
    "\n",
    "Here's how to implement a simple cost alert system:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bcda88-de4a-4f90-9262-21ad8951564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\\# Define your daily budget  \n",
    " daily\\_budget \\= 10.00  \\# $10 per day, adjust as needed\n",
    "\n",
    " \\# Check if you're approaching your limit  \n",
    " if total\\_cost \\> daily\\_budget \\* 0.8:  \n",
    " \tprint(\"Warning: 80% of daily budget reached\")  \n",
    " \t  \n",
    " \\# You might also want to implement automatic shutdown  \n",
    " if total\\_cost \\> daily\\_budget:  \n",
    " \tprint(\"Daily budget exceeded\\! Stopping further processing.\")  \n",
    " \t\\# Add logic to pause or limit operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be58a3e6-423d-48b3-a851-5e8cf41de18a",
   "metadata": {},
   "source": [
    "When your code runs, you'll see warning messages on your screen if costs approach your limits. This gives you time to review usage patterns and make adjustments before exceeding your budget.\n",
    "\n",
    "**Generating Cost Reports for Ongoing Management**\n",
    "\n",
    "Regular reporting helps you understand spending patterns and identify optimization opportunities. You can create simple reports using basic Python tools or export data for more detailed analysis.\n",
    "\n",
    "Here's how to generate a basic cost report:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4abae4-a017-4955-98fc-f8db5c2e6c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv  \n",
    " from datetime import datetime\n",
    "\n",
    " \\# Create a cost report  \n",
    " def generate\\_cost\\_report(costs\\_data):  \n",
    " \ttimestamp \\= datetime.now().strftime(\"%Y-%m-%d\\_%H-%M-%S\")  \n",
    " \tfilename \\= f\"cost\\_report\\_{timestamp}.csv\"  \n",
    " \t  \n",
    " \twith open(filename, 'w', newline='') as file:  \n",
    "     \twriter \\= csv.writer(file)  \n",
    "     \twriter.writerow(\\['Timestamp', 'Cost', 'Tokens', 'Duration'\\])  \n",
    "     \t  \n",
    "     \tfor entry in costs\\_data:  \n",
    "             writer.writerow(\\[entry\\['timestamp'\\], entry\\['cost'\\],  \n",
    "                        \tentry\\['tokens'\\], entry\\['duration'\\]\\])  \n",
    " \t  \n",
    " \tprint(f\"Cost report saved as {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77300877-1730-475c-a108-ea03583c410d",
   "metadata": {},
   "source": [
    "When you run this function, you'll find a new CSV file in your project directory with the timestamp in the filename. You can open this file in any spreadsheet application to analyze your spending patterns, identify peak usage times, and spot opportunities for further optimization.\n",
    "\n",
    "**Moving Forward with Confidence**\n",
    "\n",
    "By implementing these monitoring and optimization strategies, you're now equipped to deploy robust, powerful reasoning agents in a financially sustainable way. You have tools to track costs in real-time, automatically route queries to appropriate models, cache responses for efficiency, and monitor spending to stay within budget.\n",
    "\n",
    "Remember to review your cost reports regularly and adjust your optimization strategies based on actual usage patterns. As your application grows and evolves, these cost management techniques will help ensure your AI agent remains both powerful and economically viable.\n",
    "\n",
    "In future chapters, we'll build on these cost optimization foundations to explore more advanced deployment strategies and scaling techniques that maintain efficiency as your user base grows.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
