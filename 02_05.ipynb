{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f414b878-b816-4fc3-b7fb-41f9bfd40542",
   "metadata": {},
   "source": [
    "Benchmarking is essential for understanding exactly how well your DeepSeek reasoning agent performs compared to traditional AI models—and where you can make improvements. In this chapter, you'll learn to create comprehensive accuracy tests, run systematic comparisons, and generate clear reports that show your agent's strengths and areas for improvement.\n",
    "\n",
    "**Creating Your Test Suite**\n",
    "\n",
    "The foundation of effective benchmarking is a well-designed test suite that covers various types of reasoning challenges. You'll create a collection of test cases that represent the kinds of problems your agent will encounter in real-world use.\n",
    "\n",
    "Start by choosing a variety of reasoning challenges. You'll want to include different types of problems such as:\n",
    "\n",
    "·       Mathematical calculations and word problems\n",
    "\n",
    "·       Logical deductions and reasoning chains\n",
    "\n",
    "·       Code analysis and debugging scenarios\n",
    "\n",
    "·       Reading comprehension and inference tasks\n",
    "\n",
    "·       Creative problem-solving challenges\n",
    "\n",
    "**Documenting Your Test Cases**\n",
    "\n",
    "You'll store your test cases in a structured format that's easy for your code to process. Create a JSON file that clearly defines each test case with its input, expected output, and category.\n",
    "\n",
    "Here's how to structure your test cases:\n",
    "\\[  \n",
    "   {\"type\": \"math\", \"input\": \"25% of 80?\", \"expected\": \"20\"},  \n",
    "   {\"type\": \"logic\", \"input\": \"All cats are mammals. Fluffy is a cat. Is Fluffy a mammal?\", \"expected\": \"Yes\"},  \n",
    "   {\"type\": \"code\", \"input\": \"Find the bug: for i in range(10) print(i)\", \"expected\": \"Missing colon after range(10)\"},  \n",
    "   {\"type\": \"reasoning\", \"input\": \"If it rains, the ground gets wet. The ground is wet. Did it rain?\", \"expected\": \"Cannot determine \\- other things can make ground wet\"}  \n",
    " \\]\n",
    "\n",
    "When you create this file, save it as test\\_cases.json in your project directory. You'll be able to see this file in your file explorer, and your Python code will read from it during testing.\n",
    "\n",
    "For larger test suites, you might prefer a CSV format which you can easily edit in a spreadsheet application:\n",
    "\n",
    "type,input,expected  \n",
    " math,\"25% of 80?\",\"20\"  \n",
    " logic,\"All cats are mammals. Fluffy is a cat. Is Fluffy a mammal?\",\"Yes\"  \n",
    " code,\"Find the bug: for i in range(10) print(i)\",\"Missing colon after range(10)\"\n",
    "\n",
    "**Building Your Testing Framework**\n",
    "\n",
    "Now you'll create a Python script that systematically runs your test cases through different models and collects the results for comparison.\n",
    "\n",
    "**Setting Up the Test Runner**\n",
    "\n",
    "Create a function that loads your test cases and runs them through any model configuration you specify:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e416fadf-e846-4f0e-800d-6724bc1a025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json  \n",
    " import time\n",
    "\n",
    " def load\\_test\\_cases(filename):  \n",
    " \t\"\"\"Load test cases from a JSON file\"\"\"  \n",
    " \twith open(filename, 'r') as file:  \n",
    "     \treturn json.load(file)\n",
    "\n",
    " def run\\_test(model\\_config, test\\_cases):  \n",
    " \t\"\"\"Run all test cases through a specific model configuration\"\"\"  \n",
    " \tresults \\= \\[\\]  \n",
    " \t  \n",
    " \tfor case in test\\_cases:  \n",
    "     \tprint(f\"Running test: {case\\['input'\\]\\[:50\\]}...\")  \\# Show progress  \n",
    "     \t  \n",
    "     \t\\# Record start time  \n",
    "     \tstart\\_time \\= time.time()  \n",
    "     \t  \n",
    "     \t\\# Create and execute the task  \n",
    "     \ttask \\= Task(case\\[\"input\"\\], agent=Agent(llm\\_config=model\\_config))  \n",
    "     \tresponse \\= task.execute()  \n",
    "     \t  \n",
    "     \t\\# Calculate response time  \n",
    "     \tresponse\\_time \\= time.time() \\- start\\_time  \n",
    "     \t  \n",
    "     \t\\# Store the complete result  \n",
    "     \tresults.append({  \n",
    "         \t\"input\": case\\[\"input\"\\],  \n",
    "         \t\"response\": response.content,  \n",
    "         \t\"expected\": case\\[\"expected\"\\],  \n",
    "         \t\"type\": case\\[\"type\"\\],  \n",
    "         \t\"response\\_time\": response\\_time  \n",
    "     \t})  \n",
    "     \t  \n",
    "     \tprint(f\"  Completed in {response\\_time:.2f} seconds\")  \n",
    " \t  \n",
    " \treturn results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2612e2a-126b-4a59-b7d3-024435d55128",
   "metadata": {},
   "source": [
    "When you run this function, you'll see progress messages on your screen showing which test is currently running and how long each one takes to complete.\n",
    "\n",
    "**Implementing Response Scoring**\n",
    "\n",
    "You'll need a systematic way to evaluate whether each response is correct. Create a scoring function that compares actual responses to expected answers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a1737a-70c1-4203-9ed4-9dcd8198a449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score\\_response(response, expected, test\\_type):  \n",
    " \t\"\"\"Score a response against the expected answer\"\"\"  \n",
    " \tresponse\\_lower \\= response.lower().strip()  \n",
    " \texpected\\_lower \\= expected.lower().strip()  \n",
    " \t  \n",
    " \t\\# Basic correctness check  \n",
    " \tcorrect \\= expected\\_lower in response\\_lower  \n",
    " \t  \n",
    " \t\\# You can add more sophisticated scoring logic here  \n",
    " \tif test\\_type \\== \"math\":  \n",
    "     \t\\# For math problems, look for exact number matches  \n",
    "     \timport re  \n",
    "     \texpected\\_numbers \\= re.findall(r'\\\\d+\\\\.?\\\\d\\*', expected)  \n",
    " \t    response\\_numbers \\= re.findall(r'\\\\d+\\\\.?\\\\d\\*', response)  \n",
    "     \tcorrect \\= any(num in response\\_numbers for num in expected\\_numbers)  \n",
    " \t  \n",
    " \telif test\\_type \\== \"logic\":  \n",
    "     \t\\# For logic problems, check for yes/no or true/false  \n",
    "     \tif \"yes\" in expected\\_lower or \"true\" in expected\\_lower:  \n",
    "         \tcorrect \\= (\"yes\" in response\\_lower or \"true\" in response\\_lower)  \n",
    "     \telif \"no\" in expected\\_lower or \"false\" in expected\\_lower:  \n",
    "         \tcorrect \\= (\"no\" in response\\_lower or \"false\" in response\\_lower)  \n",
    " \t  \n",
    " \treturn {  \n",
    "     \t\"correct\": correct,  \n",
    "     \t\"confidence\": 1.0 if correct else 0.0,  \\# You can make this more nuanced  \n",
    "     \t\"response\\_length\": len(response)  \n",
    " \t}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e18099-942c-4f8e-bbe3-76e5caaa04d7",
   "metadata": {},
   "source": [
    "This function will return a dictionary with scoring information for each test case, which you'll use in your analysis.\n",
    "\n",
    "**Running Comparative Tests**\n",
    "\n",
    "Now you'll run your test suite through multiple models to compare their performance. You'll test both a baseline model and your DeepSeek reasoning agent.\n",
    "\n",
    "**Setting Up Model Configurations**\n",
    "\n",
    "First, define the configurations for the models you want to compare:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b96f64c-4032-44a1-a6d9-8614e69613eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\\# Baseline model configuration  \n",
    " baseline\\_config \\= {  \n",
    " \t\"model\": \"gpt-3.5-turbo\",  \n",
    " \t\"temperature\": 0.1,  \n",
    " \t\"max\\_tokens\": 500  \n",
    " }\n",
    "\n",
    " \\# DeepSeek reasoning agent configuration   \n",
    " reasoning\\_config \\= {  \n",
    " \t\"model\": \"deepseek-reasoner\",  \n",
    " \t\"temperature\": 0.1,  \n",
    " \t\"max\\_tokens\": 1000,  \n",
    " \t\"reasoning\\_mode\": True  \n",
    " }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f359c8-9402-4e27-a301-fa216527dd54",
   "metadata": {},
   "source": [
    "**Executing the Comparison**\n",
    "\n",
    "Load your test cases and run them through both models:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814787ec-1005-4873-b760-d99ce26cb329",
   "metadata": {},
   "outputs": [],
   "source": [
    "\\# Load your test cases  \n",
    " test\\_cases \\= load\\_test\\_cases(\"test\\_cases.json\")  \n",
    " print(f\"Loaded {len(test\\_cases)} test cases\")\n",
    "\n",
    " \\# Run baseline tests  \n",
    " print(\"\\\\nRunning baseline model tests...\")  \n",
    " baseline\\_results \\= run\\_test(baseline\\_config, test\\_cases)\n",
    "\n",
    " \\# Run reasoning agent tests  \n",
    " print(\"\\\\nRunning reasoning agent tests...\")  \n",
    " reasoning\\_results \\= run\\_test(reasoning\\_config, test\\_cases)\n",
    "\n",
    " print(\"\\\\nAll tests completed\\!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee6213d-82c3-432d-b0ba-6a25c8b56ef7",
   "metadata": {},
   "source": [
    "As this runs, you'll see progress messages telling you which model is being tested and which specific test case is currently running.\n",
    "\n",
    "**Analyzing Your Results**\n",
    "\n",
    "Once you have results from both models, you'll analyze them statistically to understand the performance differences.\n",
    "\n",
    "**Calculating Basic Statistics**\n",
    "\n",
    "Use Python's built-in statistics module to compute accuracy metrics:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d532de5d-1c18-4052-973b-f4510ab32f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    " def analyze\\_results(results, model\\_name):  \n",
    " \t\"\"\"Analyze test results and return key metrics\"\"\"  \n",
    " \t  \n",
    " \t\\# Score all responses  \n",
    " \tscores \\= \\[\\]  \n",
    " \tresponse\\_times \\= \\[\\]  \n",
    " \ttype\\_performance \\= {}  \n",
    " \t  \n",
    " \tfor result in results:  \n",
    "     \tscore \\= score\\_response(result\\[\"response\"\\], result\\[\"expected\"\\], result\\[\"type\"\\])  \n",
    "         scores.append(score\\[\"correct\"\\])  \n",
    "         response\\_times.append(result\\[\"response\\_time\"\\])  \n",
    "     \t  \n",
    "     \t\\# Track performance by test type  \n",
    "     \ttest\\_type \\= result\\[\"type\"\\]  \n",
    "         if test\\_type not in type\\_performance:  \n",
    "         \ttype\\_performance\\[test\\_type\\] \\= \\[\\]  \n",
    "         type\\_performance\\[test\\_type\\].append(score\\[\"correct\"\\])  \n",
    " \t  \n",
    " \t\\# Calculate overall metrics  \n",
    " \taccuracy \\= statistics.mean(scores)  \n",
    " \tavg\\_response\\_time \\= statistics.mean(response\\_times)  \n",
    " \t  \n",
    " \t\\# Calculate performance by type  \n",
    " \ttype\\_accuracies \\= {}  \n",
    " \tfor test\\_type, type\\_scores in type\\_performance.items():  \n",
    "     \ttype\\_accuracies\\[test\\_type\\] \\= statistics.mean(type\\_scores)  \n",
    " \t  \n",
    " \treturn {  \n",
    "     \t\"model\": model\\_name,  \n",
    "     \t\"overall\\_accuracy\": accuracy,  \n",
    "     \t\"avg\\_response\\_time\": avg\\_response\\_time,  \n",
    "     \t\"total\\_tests\": len(results),  \n",
    "     \t\"correct\\_answers\": sum(scores),  \n",
    "     \t\"type\\_accuracies\": type\\_accuracies  \n",
    " \t}\n",
    "\n",
    " \\# Analyze both sets of results  \n",
    " baseline\\_analysis \\= analyze\\_results(baseline\\_results, \"Baseline Model\")  \n",
    " reasoning\\_analysis \\= analyze\\_results(reasoning\\_results, \"DeepSeek Reasoning Agent\")\n",
    "\n",
    " \\# Calculate improvement  \n",
    " accuracy\\_improvement \\= reasoning\\_analysis\\[\"overall\\_accuracy\"\\] \\- baseline\\_analysis\\[\"overall\\_accuracy\"\\]  \n",
    " time\\_difference \\= reasoning\\_analysis\\[\"avg\\_response\\_time\"\\] \\- baseline\\_analysis\\[\"avg\\_response\\_time\"\\]\n",
    "\n",
    " print(f\"\\\\nResults Summary:\")  \n",
    " print(f\"Baseline Accuracy: {baseline\\_analysis\\['overall\\_accuracy'\\]:.1%}\")  \n",
    " print(f\"Reasoning Agent Accuracy: {reasoning\\_analysis\\['overall\\_accuracy'\\]:.1%}\")  \n",
    " print(f\"Improvement: {accuracy\\_improvement:.1%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b632c5-75b2-4082-9e6f-c8f28a40d8be",
   "metadata": {},
   "source": [
    "When you run this analysis, you'll see a clear summary printed to your screen showing the key performance differences between the models.\n",
    "\n",
    "**Creating Visual Reports**\n",
    "\n",
    "Visual reports make it much easier to understand and communicate your benchmarking results. You'll create charts and tables that clearly show performance differences.\n",
    "\n",
    "**Setting Up Data for Visualization**\n",
    "\n",
    "First, organize your data into a format that's easy to visualize:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5610bc-60a0-4371-956e-f304b9287f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    " import matplotlib.pyplot as plt\n",
    "\n",
    " def create\\_comparison\\_dataframe(baseline\\_analysis, reasoning\\_analysis):  \n",
    " \t\"\"\"Create a DataFrame for easy visualization\"\"\"  \n",
    " \t  \n",
    " \t\\# Overall comparison  \n",
    " \toverall\\_data \\= {  \n",
    "     \t\"Model\": \\[\"Baseline\", \"Reasoning Agent\"\\],  \n",
    "     \t\"Accuracy\": \\[baseline\\_analysis\\[\"overall\\_accuracy\"\\], reasoning\\_analysis\\[\"overall\\_accuracy\"\\]\\],  \n",
    "     \t\"Avg Response Time\": \\[baseline\\_analysis\\[\"avg\\_response\\_time\"\\], reasoning\\_analysis\\[\"avg\\_response\\_time\"\\]\\]  \n",
    " \t}  \n",
    " \t  \n",
    " \toverall\\_df \\= pd.DataFrame(overall\\_data)  \n",
    " \t  \n",
    " \t\\# Performance by type  \n",
    " \tall\\_types \\= set(baseline\\_analysis\\[\"type\\_accuracies\"\\].keys()) | set(reasoning\\_analysis\\[\"type\\_accuracies\"\\].keys())  \n",
    " \ttype\\_data \\= \\[\\]  \n",
    " \t  \n",
    " \tfor test\\_type in all\\_types:  \n",
    "     \tbaseline\\_acc \\= baseline\\_analysis\\[\"type\\_accuracies\"\\].get(test\\_type, 0\\)  \n",
    "     \treasoning\\_acc \\= reasoning\\_analysis\\[\"type\\_accuracies\"\\].get(test\\_type, 0\\)  \n",
    "     \t  \n",
    "     \ttype\\_data.append({  \n",
    "         \t\"Test Type\": test\\_type,  \n",
    "         \t\"Baseline\": baseline\\_acc,  \n",
    "         \t\"Reasoning Agent\": reasoning\\_acc,  \n",
    "         \t\"Improvement\": reasoning\\_acc \\- baseline\\_acc  \n",
    "     \t})  \n",
    " \t  \n",
    " \ttype\\_df \\= pd.DataFrame(type\\_data)  \n",
    " \t  \n",
    " \treturn overall\\_df, type\\_df\n",
    "\n",
    " \\# Create the dataframes  \n",
    " overall\\_df, type\\_df \\= create\\_comparison\\_dataframe(baseline\\_analysis, reasoning\\_analysis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0fbfe9-85d3-439b-9daa-4b0f20f6f7f7",
   "metadata": {},
   "source": [
    "**Generating Charts**\n",
    "\n",
    "Create clear, informative charts that highlight your findings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053053a6-fa4e-4d3c-a1c9-d6f184c8bc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\\# Create accuracy comparison chart  \n",
    " fig, (ax1, ax2) \\= plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    " \\# Overall accuracy comparison  \n",
    " overall\\_df.plot.bar(x=\"Model\", y=\"Accuracy\", ax=ax1, color=\\['lightblue', 'darkblue'\\])  \n",
    " ax1.set\\_title(\"Overall Accuracy Comparison\")  \n",
    " ax1.set\\_ylabel(\"Accuracy Rate\")  \n",
    " ax1.set\\_ylim(0, 1\\)  \n",
    " ax1.tick\\_params(axis='x', rotation=0)\n",
    "\n",
    " \\# Performance by test type  \n",
    " type\\_df.plot.bar(x=\"Test Type\", y=\\[\"Baseline\", \"Reasoning Agent\"\\], ax=ax2)  \n",
    " ax2.set\\_title(\"Accuracy by Test Type\")  \n",
    " ax2.set\\_ylabel(\"Accuracy Rate\")  \n",
    " ax2.set\\_ylim(0, 1\\)  \n",
    " ax2.tick\\_params(axis='x', rotation=45)\n",
    "\n",
    " plt.tight\\_layout()  \n",
    " plt.savefig(\"accuracy\\_comparison.png\", dpi=300, bbox\\_inches='tight')  \n",
    " plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2521020-f298-451b-a42b-fcf1cb561f8b",
   "metadata": {},
   "source": [
    "When you run this code, you'll see two bar charts appear on your screen. The first shows overall accuracy comparison, and the second breaks down performance by test type. You'll also find a high-quality PNG file saved in your project directory.\n",
    "\n",
    "**Creating a Detailed Report Table**\n",
    "\n",
    "Generate a comprehensive table that summarizes all your findings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ef45a3-4390-4ce0-ae49-80ee232aafb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate\\_detailed\\_report(baseline\\_analysis, reasoning\\_analysis):  \n",
    " \t\"\"\"Generate a detailed performance report\"\"\"  \n",
    " \t  \n",
    " \tprint(\"=\" \\* 60\\)  \n",
    " \tprint(\"BENCHMARK ACCURACY COMPARISON REPORT\")  \n",
    " \tprint(\"=\" \\* 60\\)  \n",
    " \t  \n",
    " \tprint(f\"\\\\nOVERALL PERFORMANCE:\")  \n",
    " \tprint(f\"{'Metric':\\<25} {'Baseline':\\<15} {'Reasoning Agent':\\<15} {'Change':\\<10}\")  \n",
    " \tprint(\"-\" \\* 70\\)  \n",
    " \t  \n",
    " \t\\# Overall accuracy  \n",
    " \tacc\\_change \\= reasoning\\_analysis\\[\"overall\\_accuracy\"\\] \\- baseline\\_analysis\\[\"overall\\_accuracy\"\\]  \n",
    " \tprint(f\"{'Accuracy':\\<25} {baseline\\_analysis\\['overall\\_accuracy'\\]:\\<15.1%} {reasoning\\_analysis\\['overall\\_accuracy'\\]:\\<15.1%} {acc\\_change:+.1%}\")  \n",
    " \t  \n",
    " \t\\# Response time  \n",
    " \ttime\\_change \\= reasoning\\_analysis\\[\"avg\\_response\\_time\"\\] \\- baseline\\_analysis\\[\"avg\\_response\\_time\"\\]  \n",
    " \tprint(f\"{'Avg Response Time (s)':\\<25} {baseline\\_analysis\\['avg\\_response\\_time'\\]:\\<15.2f} {reasoning\\_analysis\\['avg\\_response\\_time'\\]:\\<15.2f} {time\\_change:+.2f}\")  \n",
    " \t  \n",
    " \t\\# Total tests  \n",
    " \tprint(f\"{'Total Test Cases':\\<25} {baseline\\_analysis\\['total\\_tests'\\]:\\<15} {reasoning\\_analysis\\['total\\_tests'\\]:\\<15} {'--':\\<10}\")  \n",
    " \t  \n",
    " \tprint(f\"\\\\nPERFORMANCE BY TEST TYPE:\")  \n",
    " \tprint(f\"{'Test Type':\\<15} {'Baseline':\\<15} {'Reasoning Agent':\\<15} {'Improvement':\\<12}\")  \n",
    " \tprint(\"-\" \\* 60\\)  \n",
    " \t  \n",
    " \tfor test\\_type in baseline\\_analysis\\[\"type\\_accuracies\"\\]:  \n",
    "     \tbaseline\\_acc \\= baseline\\_analysis\\[\"type\\_accuracies\"\\]\\[test\\_type\\]  \n",
    "     \treasoning\\_acc \\= reasoning\\_analysis\\[\"type\\_accuracies\"\\].get(test\\_type, 0\\)  \n",
    "     \timprovement \\= reasoning\\_acc \\- baseline\\_acc  \n",
    "     \t  \n",
    "     \tprint(f\"{test\\_type:\\<15} {baseline\\_acc:\\<15.1%} {reasoning\\_acc:\\<15.1%} {improvement:+.1%}\")  \n",
    " \t  \n",
    " \tprint(\"=\" \\* 60\\)\n",
    "\n",
    "\\# Generate the detailed report  \n",
    " generate\\_detailed\\_report(baseline\\_analysis, reasoning\\_analysis)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8585203-35bc-469f-979e-b5ca7f476b03",
   "metadata": {},
   "source": [
    "This will print a comprehensive report to your screen that you can copy and save for documentation or sharing with others.\n",
    "\n",
    "**Interpreting and Summarizing Your Findings**\n",
    "\n",
    "After running your analysis, you'll want to create clear, actionable summaries of what you discovered.\n",
    "\n",
    "**Writing Your Summary**\n",
    "\n",
    "Based on your results, write a clear summary that highlights the key findings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a99b84-51d2-4b0c-9934-610059dcc9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create\\_executive\\_summary(baseline\\_analysis, reasoning\\_analysis):  \n",
    " \t\"\"\"Create an executive summary of benchmarking results\"\"\"  \n",
    " \t  \n",
    " \taccuracy\\_improvement \\= reasoning\\_analysis\\[\"overall\\_accuracy\"\\] \\- baseline\\_analysis\\[\"overall\\_accuracy\"\\]  \n",
    " \ttime\\_impact \\= reasoning\\_analysis\\[\"avg\\_response\\_time\"\\] \\- baseline\\_analysis\\[\"avg\\_response\\_time\"\\]  \n",
    " \t  \n",
    " \tsummary \\= f\"\"\"  \n",
    " EXECUTIVE SUMMARY:  \n",
    " The DeepSeek reasoning agent demonstrated significant performance improvements over the baseline model:\n",
    "\n",
    " • Overall accuracy increased from {baseline\\_analysis\\['overall\\_accuracy'\\]:.1%} to {reasoning\\_analysis\\['overall\\_accuracy'\\]:.1%}  \n",
    "   (improvement of {accuracy\\_improvement:+.1%})\n",
    "\n",
    " • Average response time changed by {time\\_impact:+.2f} seconds per query\n",
    "\n",
    " • Tested across {reasoning\\_analysis\\['total\\_tests'\\]} diverse test cases covering multiple reasoning types\n",
    "\n",
    " KEY INSIGHTS:  \n",
    " \"\"\"  \n",
    " \t  \n",
    " \t\\# Find best and worst performing categories  \n",
    " \timprovements\\_by\\_type \\= {}  \n",
    " \tfor test\\_type in baseline\\_analysis\\[\"type\\_accuracies\"\\]:  \n",
    "     \tbaseline\\_acc \\= baseline\\_analysis\\[\"type\\_accuracies\"\\]\\[test\\_type\\]  \n",
    "     \treasoning\\_acc \\= reasoning\\_analysis\\[\"type\\_accuracies\"\\].get(test\\_type, 0\\)  \n",
    "     \timprovements\\_by\\_type\\[test\\_type\\] \\= reasoning\\_acc \\- baseline\\_acc  \n",
    " \t  \n",
    " \tbest\\_category \\= max(improvements\\_by\\_type.keys(), key=lambda k: improvements\\_by\\_type\\[k\\])  \n",
    " \tworst\\_category \\= min(improvements\\_by\\_type.keys(), key=lambda k: improvements\\_by\\_type\\[k\\])  \n",
    " \t  \n",
    " \tsummary \\+= f\"\"\"  \n",
    " • Strongest improvement in {best\\_category} tasks ({improvements\\_by\\_type\\[best\\_category\\]:+.1%})  \n",
    " • Areas for improvement: {worst\\_category} tasks ({improvements\\_by\\_type\\[worst\\_category\\]:+.1%})\n",
    "\n",
    " RECOMMENDATIONS:  \n",
    " • Deploy reasoning agent for production use \\- shows clear accuracy benefits  \n",
    " • Monitor response time impact on user experience  \n",
    " • Consider additional training data for {worst\\_category} tasks  \n",
    " • Implement continuous benchmarking to track future improvements  \n",
    " \"\"\"  \n",
    " \t  \n",
    " \treturn summary\n",
    "\n",
    " \\# Generate and display the executive summary  \n",
    " summary \\= create\\_executive\\_summary(baseline\\_analysis, reasoning\\_analysis)  \n",
    " print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6546fffb-d7a6-4c52-93f2-310a9bf46477",
   "metadata": {},
   "source": [
    "**Setting Up Continuous Benchmarking**\n",
    "\n",
    "Benchmarking shouldn't be a one-time activity. You'll want to integrate continuous benchmarking into your development workflow to track improvements over time and make data-driven decisions about agent updates.\n",
    "\n",
    "**Creating an Automated Benchmark Runner**\n",
    "\n",
    "Set up a system that can run benchmarks automatically on a schedule:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a500ef34-edfe-4edd-80f6-a4843ac61031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime  \n",
    " import os\n",
    "\n",
    " def run\\_continuous\\_benchmark():  \n",
    " \t\"\"\"Run a complete benchmark and save results with timestamp\"\"\"  \n",
    " \t  \n",
    " \ttimestamp \\= datetime.datetime.now().strftime(\"%Y-%m-%d\\_%H-%M-%S\")  \n",
    " \tresults\\_dir \\= f\"benchmark\\_results\\_{timestamp}\"  \n",
    " \tos.makedirs(results\\_dir, exist\\_ok=True)  \n",
    " \t  \n",
    " \tprint(f\"Starting benchmark run: {timestamp}\")  \n",
    " \t  \n",
    " \t\\# Load test cases  \n",
    " \ttest\\_cases \\= load\\_test\\_cases(\"test\\_cases.json\")  \n",
    " \t  \n",
    " \t\\# Run tests  \n",
    " \tbaseline\\_results \\= run\\_test(baseline\\_config, test\\_cases)  \n",
    " \treasoning\\_results \\= run\\_test(reasoning\\_config, test\\_cases)  \n",
    " \t  \n",
    " \t\\# Analyze results  \n",
    " \tbaseline\\_analysis \\= analyze\\_results(baseline\\_results, \"Baseline\")  \n",
    " \treasoning\\_analysis \\= analyze\\_results(reasoning\\_results, \"Reasoning Agent\")  \n",
    " \t  \n",
    " \t\\# Save detailed results  \n",
    " \tresults\\_summary \\= {  \n",
    "     \t\"timestamp\": timestamp,  \n",
    "     \t\"baseline\\_analysis\": baseline\\_analysis,  \n",
    "     \t\"reasoning\\_analysis\": reasoning\\_analysis,  \n",
    "     \t\"test\\_cases\\_count\": len(test\\_cases)  \n",
    " \t}  \n",
    " \t  \n",
    " \t\\# Save to JSON file  \n",
    " \twith open(f\"{results\\_dir}/benchmark\\_results.json\", \"w\") as f:  \n",
    "     \tjson.dump(results\\_summary, f, indent=2)  \n",
    " \t  \n",
    " \t\\# Generate and save report  \n",
    " \tsummary \\= create\\_executive\\_summary(baseline\\_analysis, reasoning\\_analysis)  \n",
    " \twith open(f\"{results\\_dir}/executive\\_summary.txt\", \"w\") as f:  \n",
    "     \tf.write(summary)  \n",
    " \t  \n",
    " \tprint(f\"Benchmark completed. Results saved in {results\\_dir}/\")  \n",
    " \t  \n",
    " \treturn results\\_summary\n",
    "\n",
    " \\# Run a benchmark  \n",
    " benchmark\\_results \\= run\\_continuous\\_benchmark()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d3044e-e979-490a-897e-5fb379091a21",
   "metadata": {},
   "source": [
    "When you run this function, you'll see a new directory created with a timestamp in its name, containing all your benchmark results and reports.\n",
    "\n",
    "**Tracking Performance Over Time**\n",
    "\n",
    "Keep a history of your benchmark results to identify trends and improvements:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc478f57-b887-4a11-9ce3-2773f5807637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare\\_historical\\_results(results\\_dir=\"benchmark\\_history\"):  \n",
    " \t\"\"\"Compare current results with historical benchmarks\"\"\"  \n",
    " \t  \n",
    " \tif not os.path.exists(results\\_dir):  \n",
    "     \tprint(\"No historical results found. This will be your baseline.\")  \n",
    "     \treturn  \n",
    " \t  \n",
    " \t\\# Load all historical results  \n",
    " \thistorical\\_files \\= \\[f for f in os.listdir(results\\_dir) if f.endswith(\"benchmark\\_results.json\")\\]  \n",
    " \t  \n",
    " \tif not historical\\_files:  \n",
    "     \tprint(\"No previous benchmark files found.\")  \n",
    "     \treturn  \n",
    " \t  \n",
    " \t\\# Load the most recent previous result  \n",
    "     historical\\_files.sort(reverse=True)  \\# Most recent first  \n",
    " \tlatest\\_historical \\= historical\\_files\\[0\\]  \n",
    " \t  \n",
    " \twith open(os.path.join(results\\_dir, latest\\_historical), 'r') as f:  \n",
    "     \tprevious\\_results \\= json.load(f)  \n",
    " \t  \n",
    " \tprint(f\"Comparing with previous benchmark from: {previous\\_results\\['timestamp'\\]}\")  \n",
    " \tprint(f\"Previous accuracy: {previous\\_results\\['reasoning\\_analysis'\\]\\['overall\\_accuracy'\\]:.1%}\")  \n",
    " \t  \n",
    " \t\\# You can expand this to show trends over multiple benchmark runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1859917-f85f-420e-b654-ccc11e657e70",
   "metadata": {},
   "source": [
    "**Moving Forward with Your Benchmarking Strategy**\n",
    "\n",
    "You now have a complete benchmarking system that can measure your DeepSeek reasoning agent's performance, compare it to baseline models, and track improvements over time.\n",
    "\n",
    "Your benchmarking framework includes:\n",
    "\n",
    "·       A structured test suite covering multiple reasoning types\n",
    "\n",
    "·       Automated testing and scoring systems\n",
    "\n",
    "·       Statistical analysis and visual reporting\n",
    "\n",
    "·       Continuous monitoring capabilities\n",
    "\n",
    "Remember to regularly update your test cases to reflect new use cases and challenges your agent will face. As you make improvements to your agent, run benchmarks to verify that changes actually improve performance rather than just changing it.\n",
    "\n",
    "In upcoming chapters, we'll use these benchmarking insights to guide optimization strategies and ensure your reasoning agent continues to improve in real-world applications.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
