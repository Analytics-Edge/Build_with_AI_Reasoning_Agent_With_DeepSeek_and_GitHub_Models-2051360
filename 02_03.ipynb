{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8fdb3f7-8a87-45dd-b518-63b22d09c572",
   "metadata": {},
   "source": [
    "**What you will accomplish in this chapter:**  \n",
    " You will build a comprehensive debugging and validation system for your DeepSeek AI agent. You'll learn to identify common issues, create robust testing procedures, implement error detection mechanisms, and establish continuous improvement processes that ensure your agent performs reliably.\n",
    "\n",
    "**Understanding AI Agent Debugging**\n",
    "\n",
    "Debugging an AI reasoning agent is like being a detective and a coach rolled into one. You need sharp investigative tools to uncover where things go off-track, combined with the patience and insight to guide your agent back to clear, logical thinking.\n",
    "\n",
    "The beauty of reasoning agents is that they show their work—but this also means there are more places where things can go wrong. A traditional AI might give you a wrong answer, and you'd have no idea why. With a reasoning agent, you can see exactly which step in the logic chain broke down, making debugging both more complex and more rewarding.\n",
    "\n",
    "**Key Debugging Challenges:**\n",
    "\n",
    "·       **Reasoning Chain Errors**: Problems at any step can cascade through the entire response\n",
    "\n",
    "·       **Context Misunderstanding**: The agent might misinterpret the original question\n",
    "\n",
    "·       **Logic Gaps**: Missing steps or flawed connections between reasoning steps\n",
    "\n",
    "·       **Confidence Issues**: The agent might be uncertain but not express this clearly\n",
    "\n",
    "·       **Edge Cases**: Unusual inputs that reveal hidden weaknesses in reasoning\n",
    "\n",
    "**Step 1: Establish Comprehensive Logging System**\n",
    "\n",
    "1\\.   \t**Create a dedicated debugging module**:\n",
    "\n",
    "o   In your Codespace, create a new file called agent\\_debugger.py\n",
    "\n",
    "o   This will contain all your debugging and validation tools:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f981e9-10d4-4d1a-99be-6f36511a3ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging  \n",
    " import json  \n",
    " import traceback  \n",
    " from datetime import datetime  \n",
    " from typing import Dict, Any, List, Optional  \n",
    " from dataclasses import dataclass, asdict\n",
    "\n",
    " @dataclass  \n",
    " class DebuggingEntry:  \n",
    " \t\"\"\"Represents a single debugging log entry\"\"\"  \n",
    " \ttimestamp: str  \n",
    " \ttask\\_description: str  \n",
    " \tagent\\_response: str  \n",
    " \treasoning\\_steps: List\\[str\\]  \n",
    " \tsuccess: bool  \n",
    " \terror\\_message: Optional\\[str\\] \\= None  \n",
    " \tvalidation\\_results: Optional\\[Dict\\[str, Any\\]\\] \\= None  \n",
    " \tperformance\\_metrics: Optional\\[Dict\\[str, float\\]\\] \\= None\n",
    "\n",
    " class AgentDebugger:  \n",
    " \t\"\"\"Comprehensive debugging system for DeepSeek agent\"\"\"  \n",
    " \t  \n",
    " \tdef \\_\\_init\\_\\_(self, log\\_file: str \\= \"agent\\_debug.log\"):  \n",
    "     \t\\# Set up detailed logging  \n",
    "     \tself.log\\_file \\= log\\_file  \n",
    "     \tself.debug\\_entries \\= \\[\\]  \n",
    "     \t  \n",
    "     \t\\# Configure logging with multiple outputs  \n",
    "     \tlogging.basicConfig(  \n",
    "         \tlevel=logging.INFO,  \n",
    "         \tformat='%(asctime)s \\- %(levelname)s \\- %(message)s',  \n",
    "         \thandlers=\\[  \n",
    "                 logging.FileHandler(log\\_file, mode='a'),  \n",
    "             \tlogging.StreamHandler()  \n",
    "         \t\\]  \n",
    "     \t)  \n",
    "     \t  \n",
    "     \tself.logger \\= logging.getLogger(\\_\\_name\\_\\_)  \n",
    "     \tself.logger.info(\"🐛 Agent Debugger initialized\")  \n",
    "     \t  \n",
    "     \tprint(f\"✅ Debugging system active \\- logs saved to: {log\\_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a18e53a-2dfe-4114-87c2-e7d1b3b27877",
   "metadata": {},
   "source": [
    "2\\.   \t**Add comprehensive interaction logging**:\n",
    "\n",
    "o   Continue in agent\\_debugger.py, adding logging methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0662dd-358c-4b64-894f-4aada7556d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\tdef log\\_agent\\_interaction(self, task\\_description: str, response: str,  \n",
    "                             reasoning\\_steps: List\\[str\\], success: bool \\= True,  \n",
    "                             error\\_message: str \\= None, \\*\\*kwargs) \\-\\> DebuggingEntry:  \n",
    "     \t\"\"\"Log a complete agent interaction with all details\"\"\"  \n",
    "     \t  \n",
    "     \tentry \\= DebuggingEntry(  \n",
    "             timestamp=datetime.now().isoformat(),  \n",
    "             task\\_description=task\\_description,  \n",
    "         \tagent\\_response=response,  \n",
    "             reasoning\\_steps=reasoning\\_steps,  \n",
    "         \tsuccess=success,  \n",
    "         \terror\\_message=error\\_message,  \n",
    "             validation\\_results=kwargs.get('validation\\_results'),  \n",
    "             performance\\_metrics=kwargs.get('performance\\_metrics')  \n",
    "     \t)  \n",
    "     \t  \n",
    "     \tself.debug\\_entries.append(entry)  \n",
    "     \t  \n",
    "     \t\\# Log to file with structured information  \n",
    "     \tself.logger.info(f\"=== AGENT INTERACTION \\===\")  \n",
    "     \tself.logger.info(f\"Task: {task\\_description}\")  \n",
    "     \tself.logger.info(f\"Success: {success}\")  \n",
    "     \t  \n",
    "     \tif not success and error\\_message:  \n",
    "             self.logger.error(f\"Error: {error\\_message}\")  \n",
    "     \t  \n",
    "     \tself.logger.info(f\"Response length: {len(response)} characters\")  \n",
    "     \tself.logger.info(f\"Reasoning steps found: {len(reasoning\\_steps)}\")  \n",
    "     \t  \n",
    "     \tfor i, step in enumerate(reasoning\\_steps, 1):  \n",
    "         \tself.logger.info(f\"Step {i}: {step\\[:100\\]}{'...' if len(step) \\> 100 else ''}\")  \n",
    "     \t  \n",
    "     \tif kwargs.get('validation\\_results'):  \n",
    "             self.logger.info(f\"Validation: {kwargs\\['validation\\_results'\\]}\")  \n",
    "     \t  \n",
    "     \tself.logger.info(\"=== END INTERACTION \\===\\\\n\")  \n",
    "          \n",
    "     \treturn entry  \n",
    " \t  \n",
    " \tdef get\\_debug\\_summary(self) \\-\\> Dict\\[str, Any\\]:  \n",
    "     \t\"\"\"Get summary statistics of debugging sessions\"\"\"  \n",
    "     \tif not self.debug\\_entries:  \n",
    "         \treturn {\"message\": \"No debugging entries found\"}  \n",
    "     \t  \n",
    "     \ttotal\\_entries \\= len(self.debug\\_entries)  \n",
    "     \tsuccessful\\_entries \\= sum(1 for entry in self.debug\\_entries if entry.success)  \n",
    "     \tfailed\\_entries \\= total\\_entries \\- successful\\_entries  \n",
    "     \t  \n",
    "     \t\\# Calculate average reasoning steps  \n",
    "     \tavg\\_reasoning\\_steps \\= sum(len(entry.reasoning\\_steps) for entry in self.debug\\_entries) / total\\_entries  \n",
    "     \t  \n",
    "     \t\\# Find common error patterns  \n",
    "     \terror\\_messages \\= \\[entry.error\\_message for entry in self.debug\\_entries if entry.error\\_message\\]  \n",
    "     \t  \n",
    "     \treturn {  \n",
    "             \"total\\_interactions\": total\\_entries,  \n",
    "             \"successful\\_interactions\": successful\\_entries,  \n",
    "             \"failed\\_interactions\": failed\\_entries,  \n",
    "         \t\"success\\_rate\": successful\\_entries / total\\_entries if total\\_entries \\> 0 else 0,  \n",
    "             \"average\\_reasoning\\_steps\": avg\\_reasoning\\_steps,  \n",
    "         \t\"common\\_errors\": list(set(error\\_messages)) if error\\_messages else \\[\\]  \n",
    "     \t}  \n",
    " \t  \n",
    " \tdef export\\_debug\\_data(self, filename: str \\= None) \\-\\> str:  \n",
    "     \t\"\"\"Export debugging data to JSON file\"\"\"  \n",
    "     \tif filename is None:  \n",
    "         \tfilename \\= f\"debug\\_export\\_{datetime.now().strftime('%Y%m%d\\_%H%M%S')}.json\"  \n",
    "     \t  \n",
    "     \texport\\_data \\= {  \n",
    "         \t\"export\\_timestamp\": datetime.now().isoformat(),  \n",
    "         \t\"summary\": self.get\\_debug\\_summary(),  \n",
    "         \t\"entries\": \\[asdict(entry) for entry in self.debug\\_entries\\]  \n",
    "     \t}  \n",
    "     \t  \n",
    "     \twith open(filename, 'w') as f:  \n",
    "         \tjson.dump(export\\_data, f, indent=2)  \n",
    "     \t  \n",
    "     \tprint(f\"📁 Debug data exported to: {filename}\")  \n",
    "     \treturn filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5260b08b-2a85-474b-a93f-46153e8db816",
   "metadata": {},
   "source": [
    "**Step 2: Create Validation and Assertion Systems**\n",
    "\n",
    "1\\.   \t**Add validation methods to your debugger**:\n",
    "\n",
    "o   Continue adding to the AgentDebugger class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30b1907-5a76-45b0-b3d8-264793260f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\tdef validate\\_agent\\_response(self, expected\\_result: Any, actual\\_result: str,  \n",
    "                               validation\\_type: str \\= \"exact\\_match\") \\-\\> Dict\\[str, Any\\]:  \n",
    "     \t\"\"\"Validate agent response against expected results\"\"\"  \n",
    "     \t  \n",
    "     \tvalidation\\_result \\= {  \n",
    "         \t\"validation\\_type\": validation\\_type,  \n",
    "         \t\"expected\": str(expected\\_result),  \n",
    "         \t\"actual\": actual\\_result,  \n",
    "         \t\"passed\": False,  \n",
    "         \t\"details\": \"\"  \n",
    "     \t}  \n",
    "     \t  \n",
    "     \ttry:  \n",
    "         \tif validation\\_type \\== \"exact\\_match\":  \n",
    "                 validation\\_result\\[\"passed\"\\] \\= str(expected\\_result).strip().lower() \\== actual\\_result.strip().lower()  \n",
    "             \t  \n",
    "         \telif validation\\_type \\== \"contains\":  \n",
    "                 validation\\_result\\[\"passed\"\\] \\= str(expected\\_result).lower() in actual\\_result.lower()  \n",
    "             \t  \n",
    "         \telif validation\\_type \\== \"numeric\":  \n",
    "             \t\\# Extract numbers from response for comparison  \n",
    "             \timport re  \n",
    "             \tactual\\_numbers \\= re.findall(r'-?\\\\d+(?:\\\\.\\\\d+)?', actual\\_result)  \n",
    "             \texpected\\_numbers \\= re.findall(r'-?\\\\d+(?:\\\\.\\\\d+)?', str(expected\\_result))  \n",
    "             \t  \n",
    "             \tif actual\\_numbers and expected\\_numbers:  \n",
    "                     validation\\_result\\[\"passed\"\\] \\= float(actual\\_numbers\\[-1\\]) \\== float(expected\\_numbers\\[-1\\])  \n",
    "                     validation\\_result\\[\"details\"\\] \\= f\"Expected number: {expected\\_numbers\\[-1\\]}, Found: {actual\\_numbers\\[-1\\] if actual\\_numbers else 'None'}\"  \n",
    "             \telse:  \n",
    "                     validation\\_result\\[\"passed\"\\] \\= False  \n",
    "                     validation\\_result\\[\"details\"\\] \\= \"No numbers found in response\"  \n",
    "         \t  \n",
    "         \telif validation\\_type \\== \"letter\\_count\":  \n",
    "             \t\\# Special validation for letter counting problems  \n",
    "             \tparts \\= str(expected\\_result).split(':')  \n",
    "             \tif len(parts) \\== 2:  \n",
    "                 \tword, letter \\= parts\\[0\\].strip(), parts\\[1\\].strip()  \n",
    "                 \tactual\\_count \\= word.lower().count(letter.lower())  \n",
    "                 \t  \n",
    "                 \t\\# Extract count from agent response  \n",
    "                 \tnumbers \\= re.findall(r'\\\\d+', actual\\_result)  \n",
    "                 \tif numbers:  \n",
    "                     \tagent\\_count \\= int(numbers\\[-1\\])  \n",
    "                         validation\\_result\\[\"passed\"\\] \\= agent\\_count \\== actual\\_count  \n",
    "                         validation\\_result\\[\"details\"\\] \\= f\"Actual count: {actual\\_count}, Agent count: {agent\\_count}\"  \n",
    "                 \telse:  \n",
    "                         validation\\_result\\[\"passed\"\\] \\= False  \n",
    "                         validation\\_result\\[\"details\"\\] \\= \"No count found in agent response\"  \n",
    "         \t  \n",
    "         \tif validation\\_result\\[\"passed\"\\]:  \n",
    "             \tself.logger.info(f\"✅ Validation passed: {validation\\_type}\")  \n",
    "         \telse:  \n",
    "                 self.logger.warning(f\"❌ Validation failed: {validation\\_type} \\- {validation\\_result\\['details'\\]}\")  \n",
    "             \t  \n",
    "     \texcept Exception as e:  \n",
    "             validation\\_result\\[\"passed\"\\] \\= False  \n",
    "             validation\\_result\\[\"details\"\\] \\= f\"Validation error: {str(e)}\"  \n",
    "             self.logger.error(f\"Validation exception: {e}\")  \n",
    "     \t  \n",
    "     \treturn validation\\_result  \n",
    " \t  \n",
    " \tdef assert\\_agent\\_accuracy(self, expected\\_result: Any, actual\\_result: str,  \n",
    "                             validation\\_type: str \\= \"exact\\_match\") \\-\\> bool:  \n",
    "     \t\"\"\"Assert that agent response meets expectations \\- raises exception if not\"\"\"  \n",
    "     \tvalidation \\= self.validate\\_agent\\_response(expected\\_result, actual\\_result, validation\\_type)  \n",
    "     \t  \n",
    "     \tif not validation\\[\"passed\"\\]:  \n",
    "         \terror\\_msg \\= f\"Assertion failed: Expected '{validation\\['expected'\\]}', got '{validation\\['actual'\\]}'. {validation\\['details'\\]}\"  \n",
    "         \tself.logger.error(error\\_msg)  \n",
    "         \traise AssertionError(error\\_msg)  \n",
    "     \t  \n",
    "     \treturn True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884d10de-76c9-4130-b7a5-01cc90ba8636",
   "metadata": {},
   "source": [
    "2\\.   \t**Create specialized validation functions**:\n",
    "o   Add more specific validation methods:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb60400-4601-4b24-8823-2f5dc0b07ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\tdef validate\\_reasoning\\_quality(self, reasoning\\_steps: List\\[str\\]) \\-\\> Dict\\[str, Any\\]:  \n",
    "     \t\"\"\"Validate the quality of reasoning steps\"\"\"  \n",
    "     \tquality\\_metrics \\= {  \n",
    "         \t\"total\\_steps\": len(reasoning\\_steps),  \n",
    "             \"average\\_step\\_length\": 0,  \n",
    "         \t\"has\\_logical\\_flow\": False,  \n",
    "             \"contains\\_calculations\": False,  \n",
    "         \t\"has\\_conclusion\": False,  \n",
    "         \t\"quality\\_score\": 0.0  \n",
    "     \t}  \n",
    "     \t  \n",
    "     \tif not reasoning\\_steps:  \n",
    "             quality\\_metrics\\[\"quality\\_score\"\\] \\= 0.0  \n",
    "         \treturn quality\\_metrics  \n",
    "     \t  \n",
    "     \t\\# Calculate average step length  \n",
    "     \ttotal\\_length \\= sum(len(step) for step in reasoning\\_steps)  \n",
    "         quality\\_metrics\\[\"average\\_step\\_length\"\\] \\= total\\_length / len(reasoning\\_steps)  \n",
    "     \t  \n",
    "     \t\\# Check for logical flow indicators  \n",
    "     \tflow\\_words \\= \\['first', 'next', 'then', 'therefore', 'because', 'since', 'so'\\]  \n",
    "     \tcombined\\_text \\= ' '.join(reasoning\\_steps).lower()  \n",
    "         quality\\_metrics\\[\"has\\_logical\\_flow\"\\] \\= any(word in combined\\_text for word in flow\\_words)  \n",
    "     \t  \n",
    "     \t\\# Check for calculations  \n",
    "     \tcalculation\\_indicators \\= \\['calculate', 'add', 'subtract', 'multiply', 'divide', 'equals', '='\\]  \n",
    "         quality\\_metrics\\[\"contains\\_calculations\"\\] \\= any(indicator in combined\\_text for indicator in calculation\\_indicators)  \n",
    "     \t  \n",
    "     \t\\# Check for conclusion  \n",
    "     \tconclusion\\_words \\= \\['therefore', 'thus', 'so', 'conclusion', 'final answer', 'result'\\]  \n",
    "         quality\\_metrics\\[\"has\\_conclusion\"\\] \\= any(word in combined\\_text for word in conclusion\\_words)  \n",
    "     \t  \n",
    "     \t\\# Calculate overall quality score  \n",
    "     \tscore \\= 0  \n",
    "     \tscore \\+= min(quality\\_metrics\\[\"total\\_steps\"\\] / 5, 1\\) \\* 0.3  \\# Prefer 5+ steps  \n",
    "     \tscore \\+= (1 if quality\\_metrics\\[\"has\\_logical\\_flow\"\\] else 0\\) \\* 0.3  \n",
    "     \tscore \\+= (1 if quality\\_metrics\\[\"contains\\_calculations\"\\] else 0\\) \\* 0.2  \n",
    "     \tscore \\+= (1 if quality\\_metrics\\[\"has\\_conclusion\"\\] else 0\\) \\* 0.2  \n",
    "     \t  \n",
    "         quality\\_metrics\\[\"quality\\_score\"\\] \\= score  \n",
    "     \t  \n",
    "     \treturn quality\\_metrics  \n",
    " \t  \n",
    " \tdef detect\\_common\\_errors(self, response: str, reasoning\\_steps: List\\[str\\]) \\-\\> List\\[str\\]:  \n",
    "     \t\"\"\"Detect common reasoning errors\"\"\"  \n",
    "     \terrors \\= \\[\\]  \n",
    "     \t  \n",
    "     \t\\# Check for contradictory statements  \n",
    "     \tif \"yes\" in response.lower() and \"no\" in response.lower():  \n",
    "         \terrors.append(\"Response contains contradictory statements\")  \n",
    "     \t  \n",
    "     \t\\# Check for incomplete reasoning  \n",
    "     \tif len(reasoning\\_steps) \\< 2:  \n",
    "             errors.append(\"Insufficient reasoning steps (less than 2)\")  \n",
    "     \t  \n",
    "     \t\\# Check for circular reasoning  \n",
    "     \tfor i, step in enumerate(reasoning\\_steps):  \n",
    "         \tfor j, other\\_step in enumerate(reasoning\\_steps):  \n",
    "             \tif i \\!= j and step.lower() \\== other\\_step.lower():  \n",
    "                     errors.append(\"Circular or repetitive reasoning detected\")  \n",
    "                 \tbreak  \n",
    "     \t  \n",
    "     \t\\# Check for unsupported conclusions  \n",
    "     \tconclusion\\_words \\= \\['therefore', 'thus', 'so', 'conclusion'\\]  \n",
    "     \thas\\_conclusion \\= any(word in response.lower() for word in conclusion\\_words)  \n",
    " \t    if has\\_conclusion and len(reasoning\\_steps) \\< 3:  \n",
    "             errors.append(\"Conclusion drawn without sufficient reasoning steps\")  \n",
    "     \t  \n",
    "     \treturn errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfc05dd-e479-4a13-8845-ecfd8ff2b923",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Step 3: Build Edge Case Testing System**\n",
    "\n",
    "1\\.   \t**Create comprehensive test cases**:\n",
    "\n",
    "o   Create a new file called agent\\_test\\_suite.py:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e315a1c0-0465-48e9-b9e9-d7c0bf3e5726",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepseek\\_agent import reasoning\\_agent\\_enhanced  \n",
    " from agent\\_debugger import AgentDebugger  \n",
    " from reasoning\\_extractor import ReasoningExtractor  \n",
    " from typing import List, Dict, Any\n",
    "\n",
    " class AgentTestSuite:  \n",
    " \t\"\"\"Comprehensive test suite for DeepSeek agent\"\"\"  \n",
    " \t  \n",
    " \tdef \\_\\_init\\_\\_(self):  \n",
    "     \tself.debugger \\= AgentDebugger(\"test\\_debug.log\")  \n",
    "     \tself.extractor \\= ReasoningExtractor()  \n",
    "     \tself.test\\_results \\= \\[\\]  \n",
    "     \tprint(\"🧪 Agent Test Suite initialized\")  \n",
    " \t  \n",
    " \tdef run\\_edge\\_case\\_tests(self) \\-\\> Dict\\[str, Any\\]:  \n",
    "     \t\"\"\"Run comprehensive edge case testing\"\"\"  \n",
    "     \tprint(\"🎯 Running Edge Case Tests...\")  \n",
    "     \tprint(\"=\"\\*50)  \n",
    "     \t  \n",
    "     \tedge\\_cases \\= \\[  \n",
    "         \t\\# Ambiguous questions  \n",
    "         \t{  \n",
    "             \t\"name\": \"Ambiguous Question\",  \n",
    "             \t\"question\": \"How long is a piece of string?\",  \n",
    "                 \"expected\\_behavior\": \"should ask for clarification or explain ambiguity\"  \n",
    "         \t},  \n",
    "         \t  \n",
    "         \t\\# Missing information  \n",
    "         \t{  \n",
    "             \t\"name\": \"Missing Information\",  \n",
    "             \t\"question\": \"Calculate the area of a rectangle with width 5.\",  \n",
    "                 \"expected\\_behavior\": \"should identify missing height/length\"  \n",
    "         \t},  \n",
    "         \t  \n",
    "         \t\\# Contradictory data  \n",
    "         \t{  \n",
    "             \t\"name\": \"Contradictory Information\",  \n",
    "             \t\"question\": \"If all birds can fly, and penguins are birds, but penguins cannot fly, what's the conclusion?\",  \n",
    "                 \"expected\\_behavior\": \"should identify the logical contradiction\"  \n",
    "         \t},  \n",
    "         \t  \n",
    "         \t\\# Mathematical edge cases  \n",
    "         \t{  \n",
    "                 \"name\": \"Division by Zero\",  \n",
    "             \t\"question\": \"What is 10 divided by 0? Show your reasoning.\",  \n",
    "                 \"expected\\_behavior\": \"should explain why division by zero is undefined\"  \n",
    "         \t},  \n",
    "         \t  \n",
    "         \t\\# Very simple problems  \n",
    "         \t{  \n",
    "             \t\"name\": \"Trivial Problem\",  \n",
    "             \t\"question\": \"What is 1 \\+ 1?\",  \n",
    "                 \"expected\\_behavior\": \"should provide reasoning even for simple problems\"  \n",
    "         \t},  \n",
    "         \t  \n",
    "         \t\\# Complex multi-part problems  \n",
    "         \t{  \n",
    "             \t\"name\": \"Complex Multi-part\",  \n",
    "             \t\"question\": \"A train travels 60 mph for 2 hours, then 80 mph for 1.5 hours. What's the average speed for the entire journey?\",  \n",
    "                 \"expected\\_behavior\": \"should break down into clear steps\"  \n",
    "         \t}  \n",
    "     \t\\]  \n",
    "     \t  \n",
    "     \tresults \\= \\[\\]  \n",
    "     \t  \n",
    "     \tfor i, test\\_case in enumerate(edge\\_cases, 1):  \n",
    "             print(f\"\\\\n\\[{i}/{len(edge\\_cases)}\\] Testing: {test\\_case\\['name'\\]}\")  \n",
    "         \tprint(f\"Question: {test\\_case\\['question'\\]}\")  \n",
    "         \tprint(f\"Expected: {test\\_case\\['expected\\_behavior'\\]}\")  \n",
    "         \tprint(\"-\" \\* 40\\)  \n",
    "         \t  \n",
    "         \ttry:  \n",
    "             \t\\# Get agent response  \n",
    "             \tresult \\= reasoning\\_agent\\_enhanced.ask\\_question(  \n",
    "                     test\\_case\\['question'\\],  \n",
    "                 \tshow\\_reasoning=False  \n",
    "             \t)  \n",
    "             \t  \n",
    "             \tif result\\['status'\\] \\== 'success':  \n",
    "                 \t\\# Extract reasoning steps  \n",
    "                 \tsummary \\= self.extractor.get\\_reasoning\\_summary(result\\['response'\\])  \n",
    "                 \t  \n",
    "                 \t\\# Analyze the response  \n",
    "                 \ttest\\_result \\= {  \n",
    "                         \"test\\_name\": test\\_case\\['name'\\],  \n",
    "                         \"question\": test\\_case\\['question'\\],  \n",
    "                         \"response\": result\\['response'\\],  \n",
    "                         \"reasoning\\_steps\": len(summary\\['reasoning\\_steps'\\]),  \n",
    "                         \"quality\\_score\": summary\\['quality\\_metrics'\\]\\['quality\\_score'\\],  \n",
    "                     \t\"errors\\_detected\": self.debugger.detect\\_common\\_errors(  \n",
    "                             result\\['response'\\],  \n",
    "                             \\[step\\['content'\\] for step in summary\\['reasoning\\_steps'\\]\\]  \n",
    "                     \t),  \n",
    "                         \"passed\": True  \\# We'll evaluate this manually for edge cases  \n",
    "                 \t}  \n",
    "                 \t  \n",
    "                 \t\\# Log the interaction  \n",
    "                     self.debugger.log\\_agent\\_interaction(  \n",
    "                         test\\_case\\['question'\\],  \n",
    "                     \tresult\\['response'\\],  \n",
    "                     \t\\[step\\['content'\\] for step in summary\\['reasoning\\_steps'\\]\\],  \n",
    "                     \tsuccess=True  \n",
    "                 \t)  \n",
    "                 \t  \n",
    "                 \tprint(f\"✅ Response received ({test\\_result\\['reasoning\\_steps'\\]} steps)\")  \n",
    "                 \tprint(f\"Quality score: {test\\_result\\['quality\\_score'\\]:.2f}\")  \n",
    "                 \t  \n",
    "                 \tif test\\_result\\['errors\\_detected'\\]:  \n",
    "                     \tprint(f\"⚠️ Errors detected: {', '.join(test\\_result\\['errors\\_detected'\\])}\")  \n",
    "                 \telse:  \n",
    "                     \tprint(\"✅ No obvious errors detected\")  \n",
    "                 \t  \n",
    "             \telse:  \n",
    "                 \ttest\\_result \\= {  \n",
    "                         \"test\\_name\": test\\_case\\['name'\\],  \n",
    "                         \"question\": test\\_case\\['question'\\],  \n",
    "                         \"response\": result\\['response'\\],  \n",
    "                         \"reasoning\\_steps\": 0,  \n",
    "                         \"quality\\_score\": 0.0,  \n",
    "                         \"errors\\_detected\": \\[\"Agent request failed\"\\],  \n",
    "                         \"passed\": False  \n",
    "                 \t}  \n",
    "      \t             \n",
    "                 \tprint(f\"❌ Agent failed: {result\\['response'\\]}\")  \n",
    "             \t  \n",
    "                 results.append(test\\_result)  \n",
    "             \t  \n",
    "         \texcept Exception as e:  \n",
    "             \tprint(f\"❌ Test exception: {str(e)}\")  \n",
    "             \tresults.append({  \n",
    "                     \"test\\_name\": test\\_case\\['name'\\],  \n",
    "                 \t\"question\": test\\_case\\['question'\\],  \n",
    "                 \t\"response\": f\"Exception: {str(e)}\",  \n",
    "                     \"reasoning\\_steps\": 0,  \n",
    "                     \"quality\\_score\": 0.0,  \n",
    "                     \"errors\\_detected\": \\[f\"Exception: {str(e)}\"\\],  \n",
    "                 \t\"passed\": False  \n",
    "             \t})  \n",
    "     \t  \n",
    "     \t\\# Generate summary report  \n",
    "     \ttotal\\_tests \\= len(results)  \n",
    "   \t  successful\\_tests \\= sum(1 for r in results if r\\['passed'\\])  \n",
    "     \tavg\\_quality \\= sum(r\\['quality\\_score'\\] for r in results) / total\\_tests if total\\_tests \\> 0 else 0  \n",
    "     \t  \n",
    "     \tsummary\\_report \\= {  \n",
    "         \t\"total\\_tests\": total\\_tests,  \n",
    "         \t\"successful\\_tests\": successful\\_tests,  \n",
    "         \t\"success\\_rate\": successful\\_tests / total\\_tests if total\\_tests \\> 0 else 0,  \n",
    "             \"average\\_quality\\_score\": avg\\_quality,  \n",
    "         \t\"detailed\\_results\": results  \n",
    "     \t}  \n",
    "     \t  \n",
    "     \tprint(f\"\\\\n{'='\\*50}\")  \n",
    "     \tprint(f\"🏆 EDGE CASE TEST SUMMARY:\")  \n",
    "     \tprint(f\"   Tests run: {total\\_tests}\")  \n",
    "     \tprint(f\"   Success rate: {summary\\_report\\['success\\_rate'\\]:.2%}\")  \n",
    "     \tprint(f\"   Average quality: {avg\\_quality:.2f}\")  \n",
    "     \t  \n",
    "     \treturn summary\\_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06185887-fa93-4b8a-a739-fdaa32957bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "2\\.   \t**Add specific validation tests**:\n",
    "\n",
    "o   Continue in agent\\_test\\_suite.py:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780bef0f-7f1f-427b-b1d3-8892d0707e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\tdef run\\_validation\\_tests(self) \\-\\> Dict\\[str, Any\\]:  \n",
    "     \t\"\"\"Run tests with known correct answers for validation\"\"\"  \n",
    "     \tprint(\"\\\\n🔍 Running Validation Tests...\")  \n",
    "     \tprint(\"=\"\\*50)  \n",
    "     \t  \n",
    "     \tvalidation\\_tests \\= \\[  \n",
    "         \t{  \n",
    "             \t\"question\": \"How many R's are in the word 'strawberry'?\",  \n",
    "             \t\"expected\": \"strawberry:r\",  \\# Special format for letter counting  \n",
    "                 \"validation\\_type\": \"letter\\_count\"  \n",
    "         \t},  \n",
    "         \t{  \n",
    "             \t\"question\": \"What is 15% of 200?\",  \n",
    "             \t\"expected\": \"30\",  \n",
    "                 \"validation\\_type\": \"numeric\"  \n",
    "         \t},  \n",
    "         \t{  \n",
    "          \t   \"question\": \"If you buy 3 apples at $2 each and 2 oranges at $1.50 each, what's the total cost?\",  \n",
    "             \t\"expected\": \"9\",  \\# $6 \\+ $3 \\= $9  \n",
    "                 \"validation\\_type\": \"numeric\"  \n",
    "         \t},  \n",
    "         \t{  \n",
    "             \t\"question\": \"Is Paris the capital of France?\",  \n",
    "             \t\"expected\": \"yes\",  \n",
    "                 \"validation\\_type\": \"contains\"  \n",
    "         \t}  \n",
    "     \t\\]  \n",
    "     \t  \n",
    "     \tvalidation\\_results \\= \\[\\]  \n",
    "     \t  \n",
    "     \tfor i, test in enumerate(validation\\_tests, 1):  \n",
    "         \tprint(f\"\\\\n\\[{i}/{len(validation\\_tests)}\\] Validating: {test\\['question'\\]}\")  \n",
    "         \t  \n",
    "         \ttry:  \n",
    "             \t\\# Get agent response  \n",
    "             \tresult \\= reasoning\\_agent\\_enhanced.ask\\_question(test\\['question'\\], show\\_reasoning=False)  \n",
    "             \t  \n",
    "             \tif result\\['status'\\] \\== 'success':  \n",
    "                 \t\\# Validate the response  \n",
    "                 \tvalidation \\= self.debugger.validate\\_agent\\_response(  \n",
    "                     \ttest\\['expected'\\],  \n",
    "                         result\\['response'\\],  \n",
    "                         test\\['validation\\_type'\\]  \n",
    "                 \t)  \n",
    "                 \t  \n",
    "                 \t\\# Extract reasoning for quality assessment  \n",
    "                 \tsummary \\= self.extractor.get\\_reasoning\\_summary(result\\['response'\\])  \n",
    "                 \t  \n",
    "                 \tvalidation\\_result \\= {  \n",
    "                         \"question\": test\\['question'\\],  \n",
    "                         \"expected\": test\\['expected'\\],  \n",
    "                         \"actual\\_response\": result\\['response'\\],  \n",
    "                         \"validation\\_passed\": validation\\['passed'\\],  \n",
    "                         \"validation\\_details\": validation\\['details'\\],  \n",
    "                         \"reasoning\\_steps\": len(summary\\['reasoning\\_steps'\\]),  \n",
    "                         \"quality\\_score\": summary\\['quality\\_metrics'\\]\\['quality\\_score'\\]  \n",
    "                 \t}  \n",
    "                 \t  \n",
    "                 \t\\# Log the result  \n",
    "                     self.debugger.log\\_agent\\_interaction(  \n",
    "                     \ttest\\['question'\\],  \n",
    "                         result\\['response'\\],  \n",
    "                     \t\\[step\\['content'\\] for step in summary\\['reasoning\\_steps'\\]\\],  \n",
    "                         success=validation\\['passed'\\],  \n",
    "                         validation\\_results=validation  \n",
    "                 \t)  \n",
    "                 \t  \n",
    "                 \tstatus \\= \"✅ PASS\" if validation\\['passed'\\] else \"❌ FAIL\"  \n",
    "                 \tprint(f\"   {status} \\- {validation.get('details', 'No details')}\")  \n",
    "                 \t  \n",
    "             \telse:  \n",
    "                 \tvalidation\\_result \\= {  \n",
    "                         \"question\": test\\['question'\\],  \n",
    "                         \"expected\": test\\['expected'\\],  \n",
    "                         \"actual\\_response\": result\\['response'\\],  \n",
    "                         \"validation\\_passed\": False,  \n",
    "                         \"validation\\_details\": \"Agent request failed\",  \n",
    "                         \"reasoning\\_steps\": 0,  \n",
    "                         \"quality\\_score\": 0.0  \n",
    "                 \t}  \n",
    "                 \t  \n",
    "                 \tprint(f\"   ❌ FAIL \\- Agent error: {result\\['response'\\]}\")  \n",
    "             \t  \n",
    "                 validation\\_results.append(validation\\_result)  \n",
    "             \t  \n",
    "         \texcept Exception as e:  \n",
    "             \tprint(f\"   ❌ EXCEPTION \\- {str(e)}\")  \n",
    "                 validation\\_results.append({  \n",
    "                 \t\"question\": test\\['question'\\],  \n",
    "                 \t\"expected\": test\\['expected'\\],  \n",
    "                     \"actual\\_response\": f\"Exception: {str(e)}\",  \n",
    "                     \"validation\\_passed\": False,  \n",
    "                     \"validation\\_details\": f\"Test exception: {str(e)}\",  \n",
    "                     \"reasoning\\_steps\": 0,  \n",
    "                     \"quality\\_score\": 0.0  \n",
    "             \t})  \n",
    "     \t  \n",
    "     \t\\# Calculate summary statistics  \n",
    "     \ttotal\\_validations \\= len(validation\\_results)  \n",
    "     \tpassed\\_validations \\= sum(1 for r in validation\\_results if r\\['validation\\_passed'\\])  \n",
    "     \tavg\\_quality \\= sum(r\\['quality\\_score'\\] for r in validation\\_results) / total\\_validations if total\\_validations \\> 0 else 0  \n",
    "     \t  \n",
    "     \tvalidation\\_summary \\= {  \n",
    "         \t\"total\\_tests\": total\\_validations,  \n",
    "         \t\"passed\\_tests\": passed\\_validations,  \n",
    "             \"validation\\_accuracy\": passed\\_validations / total\\_validations if total\\_validations \\> 0 else 0,  \n",
    "         \t\"average\\_quality\\_score\": avg\\_quality,  \n",
    "         \t\"detailed\\_results\": validation\\_results  \n",
    "     \t}  \n",
    "     \t  \n",
    "     \tprint(f\"\\\\n📊 VALIDATION TEST SUMMARY:\")  \n",
    "     \tprint(f\"   Accuracy: {validation\\_summary\\['validation\\_accuracy'\\]:.2%} ({passed\\_validations}/{total\\_validations})\")  \n",
    "     \tprint(f\"   Average quality: {avg\\_quality:.2f}\")  \n",
    "     \t  \n",
    "     \treturn validation\\_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e07c78-50cc-4f24-943f-947591ae68b9",
   "metadata": {},
   "source": [
    "**Step 4: Implement Continuous Improvement System**\n",
    "\n",
    "1\\.   \t**Create improvement tracking system**:\n",
    "\n",
    "o   Create improvement\\_tracker.py:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0a81dd-4a20-45bc-b04d-88c63151e503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json  \n",
    " from datetime import datetime  \n",
    " from typing import Dict, List, Any  \n",
    " from agent\\_debugger import AgentDebugger\n",
    "\n",
    " class ImprovementTracker:  \n",
    " \t\"\"\"Tracks agent improvements over time\"\"\"  \n",
    " \t  \n",
    " \tdef \\_\\_init\\_\\_(self, history\\_file: str \\= \"improvement\\_history.json\"):  \n",
    "     \tself.history\\_file \\= history\\_file  \n",
    "     \tself.improvement\\_history \\= self.load\\_history()  \n",
    "     \tprint(\"📈 Improvement Tracker initialized\")  \n",
    " \t  \n",
    " \tdef load\\_history(self) \\-\\> List\\[Dict\\[str, Any\\]\\]:  \n",
    "     \t\"\"\"Load improvement history from file\"\"\"  \n",
    "     \ttry:  \n",
    "         \twith open(self.history\\_file, 'r') as f:  \n",
    "             \treturn json.load(f)  \n",
    "     \texcept FileNotFoundError:  \n",
    "         \treturn \\[\\]  \n",
    "     \texcept json.JSONDecodeError:  \n",
    "         \tprint(f\"⚠️ Warning: Could not parse {self.history\\_file}, starting fresh\")  \n",
    "         \treturn \\[\\]  \n",
    " \t  \n",
    " \tdef save\\_history(self):  \n",
    "     \t\"\"\"Save improvement history to file\"\"\"  \n",
    "     \twith open(self.history\\_file, 'w') as f:  \n",
    "             json.dump(self.improvement\\_history, f, indent=2)  \n",
    " \t  \n",
    " \tdef record\\_improvement\\_session(self, test\\_results: Dict\\[str, Any\\],  \n",
    "                                  improvements\\_made: List\\[str\\] \\= None):  \n",
    "     \t\"\"\"Record the results of an improvement session\"\"\"  \n",
    "     \t  \n",
    "     \tsession\\_record \\= {  \n",
    "         \t\"timestamp\": datetime.now().isoformat(),  \n",
    "         \t\"test\\_summary\": {  \n",
    "             \t\"total\\_tests\": test\\_results.get('total\\_tests', 0),  \n",
    "             \t\"success\\_rate\": test\\_results.get('success\\_rate', 0),  \n",
    "                 \"validation\\_accuracy\": test\\_results.get('validation\\_accuracy', 0),  \n",
    "                 \"average\\_quality\\_score\": test\\_results.get('average\\_quality\\_score', 0\\)  \n",
    "         \t},  \n",
    "             \"improvements\\_made\": improvements\\_made or \\[\\],  \n",
    "         \t\"notes\": \"\"  \n",
    "     \t}  \n",
    "     \t  \n",
    "         self.improvement\\_history.append(session\\_record)  \n",
    "     \tself.save\\_history()  \n",
    "     \t  \n",
    "     \tprint(f\"📝 Improvement session recorded\")  \n",
    "     \t  \n",
    " \tdef analyze\\_improvement\\_trends(self) \\-\\> Dict\\[str, Any\\]:  \n",
    "     \t\"\"\"Analyze improvement trends over time\"\"\"  \n",
    "     \tif len(self.improvement\\_history) \\< 2:  \n",
    "         \treturn {\"message\": \"Need at least 2 sessions to analyze trends\"}  \n",
    "     \t  \n",
    "     \t\\# Get latest and previous sessions  \n",
    "     \tlatest \\= self.improvement\\_history\\[-1\\]\\['test\\_summary'\\]  \n",
    "     \tprevious \\= self.improvement\\_history\\[-2\\]\\['test\\_summary'\\]  \n",
    "     \t  \n",
    "     \ttrends \\= {  \n",
    "         \t\"success\\_rate\\_change\": latest\\['success\\_rate'\\] \\- previous\\['success\\_rate'\\],  \n",
    "         \t\"accuracy\\_change\": latest.get('validation\\_accuracy', 0\\) \\- previous.get('validation\\_accuracy', 0),  \n",
    "         \t\"quality\\_change\": latest\\['average\\_quality\\_score'\\] \\- previous\\['average\\_quality\\_score'\\],  \n",
    "         \t\"total\\_sessions\": len(self.improvement\\_history),  \n",
    "             \"improvements\\_over\\_time\": \\[session\\['improvements\\_made'\\] for session in self.improvement\\_history\\]  \n",
    "     \t}  \n",
    "     \t  \n",
    "     \t\\# Determine overall trend  \n",
    "     \timprovements \\= sum(\\[  \n",
    "         \t1 if trends\\['success\\_rate\\_change'\\] \\> 0 else 0,  \n",
    "         \t1 if trends\\['accuracy\\_change'\\] \\> 0 else 0,  \n",
    "         \t1 if trends\\['quality\\_change'\\] \\> 0 else 0  \n",
    "     \t\\])  \n",
    "     \t  \n",
    "     \tif improvements \\>= 2:  \n",
    "         \ttrends\\['overall\\_trend'\\] \\= \"improving\"  \n",
    "     \telif improvements \\>= 1:  \n",
    "         \ttrends\\['overall\\_trend'\\] \\= \"mixed\"  \n",
    "     \telse:  \n",
    "         \ttrends\\['overall\\_trend'\\] \\= \"declining\"  \n",
    "     \t  \n",
    "     \treturn trends  \n",
    " \t  \n",
    " \tdef suggest\\_improvements(self, test\\_results: Dict\\[str, Any\\]) \\-\\> List\\[str\\]:  \n",
    "     \t\"\"\"Suggest specific improvements based on test results\"\"\"  \n",
    "     \tsuggestions \\= \\[\\]  \n",
    "     \t  \n",
    "     \t\\# Based on success rate  \n",
    "     \tif test\\_results.get('success\\_rate', 1\\) \\< 0.8:  \n",
    "             suggestions.append(\"Consider revising prompt templates for better clarity\")  \n",
    "         \tsuggestions.append(\"Add more specific instructions for reasoning steps\")  \n",
    "     \t  \n",
    "     \t\\# Based on validation accuracy  \n",
    "     \tif test\\_results.get('validation\\_accuracy', 1\\) \\< 0.8:  \n",
    "         \tsuggestions.append(\"Implement more robust validation checks\")  \n",
    "         \tsuggestions.append(\"Add specific examples for common problem types\")  \n",
    "     \t  \n",
    "     \t\\# Based on quality scores  \n",
    "     \tif test\\_results.get('average\\_quality\\_score', 1\\) \\< 0.7:  \n",
    "             suggestions.append(\"Encourage more detailed step-by-step explanations\")  \n",
    "         \tsuggestions.append(\"Add prompts that specifically ask for reasoning justification\")  \n",
    "     \t  \n",
    "     \t\\# Based on historical trends  \n",
    "     \ttrends \\= self.analyze\\_improvement\\_trends()  \n",
    "         if isinstance(trends, dict) and trends.get('overall\\_trend') \\== 'declining':  \n",
    "             suggestions.append(\"Review recent changes that may have negatively impacted performance\")  \n",
    "             suggestions.append(\"Consider reverting to a previous configuration\")  \n",
    "     \t  \n",
    "     \treturn suggestions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0887f962-f6bb-45ed-b39f-98afc616499d",
   "metadata": {},
   "source": [
    "**Step 5: Create Comprehensive Testing and Improvement Workflow**\n",
    "\n",
    "1\\.   \t**Create the main testing and improvement script**:\n",
    "\n",
    "o   Create run\\_comprehensive\\_debug.py:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caded99f-c66b-4c90-94d4-1470e44dca67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent\\_test\\_suite import AgentTestSuite  \n",
    " from improvement\\_tracker import ImprovementTracker  \n",
    " from agent\\_debugger import AgentDebugger  \n",
    " import json\n",
    "\n",
    " def run\\_comprehensive\\_debugging\\_session():  \n",
    " \t\"\"\"Run a complete debugging and improvement session\"\"\"  \n",
    " \tprint(\"🔧 COMPREHENSIVE DEBUGGING SESSION\")  \n",
    " \tprint(\"=\"\\*60)  \n",
    " \tprint(\"This will test your agent thoroughly and suggest improvements.\")  \n",
    " \tprint(\"-\"\\*60)  \n",
    " \t  \n",
    " \t\\# Initialize systems  \n",
    " \ttest\\_suite \\= AgentTestSuite()  \n",
    " \timprovement\\_tracker \\= ImprovementTracker()  \n",
    " \tdebugger \\= AgentDebugger(\"comprehensive\\_debug.log\")  \n",
    " \t  \n",
    " \t\\# Run all tests  \n",
    " \tprint(\"\\\\n🎯 Phase 1: Edge Case Testing\")  \n",
    " \tedge\\_case\\_results \\= test\\_suite.run\\_edge\\_case\\_tests()  \n",
    " \t  \n",
    " \tprint(\"\\\\n🔍 Phase 2: Validation Testing\")  \n",
    " \tvalidation\\_results \\= test\\_suite.run\\_validation\\_tests()  \n",
    " \t  \n",
    " \t\\# Combine results  \n",
    " \tcomprehensive\\_results \\= {  \n",
    "     \t\"edge\\_case\\_results\": edge\\_case\\_results,  \n",
    "     \t\"validation\\_results\": validation\\_results,  \n",
    "     \t\"combined\\_metrics\": {  \n",
    "             \"overall\\_success\\_rate\": (edge\\_case\\_results.get('success\\_rate', 0\\) \\+  \n",
    "                                    validation\\_results.get('validation\\_accuracy', 0)) / 2,  \n",
    "             \"combined\\_quality\\_score\": (edge\\_case\\_results.get('average\\_quality\\_score', 0\\) \\+  \n",
    "                                      validation\\_results.get('average\\_quality\\_score', 0)) / 2  \n",
    "     \t}  \n",
    " \t}  \n",
    " \t  \n",
    " \t\\# Get improvement suggestions  \n",
    " \tprint(\"\\\\n💡 Phase 3: Improvement Analysis\")  \n",
    " \tsuggestions \\= improvement\\_tracker.suggest\\_improvements(comprehensive\\_results\\['combined\\_metrics'\\])  \n",
    " \t  \n",
    " \tprint(\"📋 IMPROVEMENT SUGGESTIONS:\")  \n",
    " \tif suggestions:  \n",
    "     \tfor i, suggestion in enumerate(suggestions, 1):  \n",
    "         \tprint(f\"   {i}. {suggestion}\")  \n",
    " \telse:  \n",
    "     \tprint(\"   🎉 No major improvements needed\\! Your agent is performing well.\")  \n",
    " \t  \n",
    " \t\\# Get debugging summary  \n",
    " \tdebug\\_summary \\= debugger.get\\_debug\\_summary()  \n",
    " \t  \n",
    " \t\\# Show comprehensive report  \n",
    " \tprint(f\"\\\\n{'='\\*60}\")  \n",
    " \tprint(\"📊 COMPREHENSIVE DEBUGGING REPORT\")  \n",
    " \tprint(\"=\"\\*60)  \n",
    " \t  \n",
    " \tprint(f\"🎯 Edge Case Performance:\")  \n",
    " \tprint(f\"   Success Rate: {edge\\_case\\_results.get('success\\_rate', 0):.2%}\")  \n",
    " \tprint(f\"   Average Quality: {edge\\_case\\_results.get('average\\_quality\\_score', 0):.2f}\")  \n",
    " \t  \n",
    " \tprint(f\"\\\\n🔍 Validation Performance:\")  \n",
    " \tprint(f\"   Accuracy: {validation\\_results.get('validation\\_accuracy', 0):.2%}\")  \n",
    " \tprint(f\"   Quality Score: {validation\\_results.get('average\\_quality\\_score', 0):.2f}\")  \n",
    " \t  \n",
    " \tprint(f\"\\\\n📈 Overall Metrics:\")  \n",
    " \tprint(f\"   Combined Success Rate: {comprehensive\\_results\\['combined\\_metrics'\\]\\['overall\\_success\\_rate'\\]:.2%}\")  \n",
    " \tprint(f\"   Combined Quality Score: {comprehensive\\_results\\['combined\\_metrics'\\]\\['combined\\_quality\\_score'\\]:.2f}\")  \n",
    " \t  \n",
    " \tprint(f\"\\\\n🐛 Debug Statistics:\")  \n",
    " \tprint(f\"   Total Interactions: {debug\\_summary.get('total\\_interactions', 0)}\")  \n",
    " \tprint(f\"   Success Rate: {debug\\_summary.get('success\\_rate', 0):.2%}\")  \n",
    " \tprint(f\"   Average Reasoning Steps: {debug\\_summary.get('average\\_reasoning\\_steps', 0):.1f}\")  \n",
    " \t  \n",
    " \t\\# Record this session  \n",
    "     improvement\\_tracker.record\\_improvement\\_session(  \n",
    "         comprehensive\\_results\\['combined\\_metrics'\\],  \n",
    "     \timprovements\\_made=\\[\\]  \\# User can fill this in manually  \n",
    " \t)  \n",
    " \t  \n",
    " \t\\# Export detailed results  \n",
    " \texport\\_filename \\= debugger.export\\_debug\\_data()  \n",
    " \t  \n",
    " \tprint(f\"\\\\n💾 Session Results:\")  \n",
    " \tprint(f\"   Detailed debug log: comprehensive\\_debug.log\")  \n",
    " \tprint(f\"   Exported data: {export\\_filename}\")  \n",
    " \tprint(f\"   Improvement history: improvement\\_history.json\")  \n",
    " \t  \n",
    " \tprint(f\"\\\\n🎉 Comprehensive debugging session completed\\!\")  \n",
    " \t  \n",
    " \treturn comprehensive\\_results\n",
    "\n",
    " def interactive\\_debugging\\_menu():  \n",
    " \t\"\"\"Interactive menu for debugging options\"\"\"  \n",
    " \twhile True:  \n",
    "     \tprint(\"\\\\n🔧 DEBUGGING MENU\")  \n",
    "     \tprint(\"=\"\\*30)  \n",
    "     \tprint(\"1. Run comprehensive debugging session\")  \n",
    "     \tprint(\"2. Run edge case tests only\")  \n",
    "     \tprint(\"3. Run validation tests only\")  \n",
    "     \tprint(\"4. View improvement history\")  \n",
    "     \tprint(\"5. Export debug data\")  \n",
    "     \tprint(\"6. Exit\")  \n",
    "     \t  \n",
    "     \tchoice \\= input(\"\\\\nSelect option (1-6): \").strip()  \n",
    "     \t  \n",
    "     \tif choice \\== \"1\":  \n",
    "             run\\_comprehensive\\_debugging\\_session()  \n",
    "     \telif choice \\== \"2\":  \n",
    "         \ttest\\_suite \\= AgentTestSuite()  \n",
    "             test\\_suite.run\\_edge\\_case\\_tests()  \n",
    "     \telif choice \\== \"3\":  \n",
    "         \ttest\\_suite \\= AgentTestSuite()  \n",
    "             test\\_suite.run\\_validation\\_tests()  \n",
    "     \telif choice \\== \"4\":  \n",
    "         \ttracker \\= ImprovementTracker()  \n",
    "         \ttrends \\= tracker.analyze\\_improvement\\_trends()  \n",
    "         \tprint(f\"\\\\n📈 Improvement Trends: {json.dumps(trends, indent=2)}\")  \n",
    "     \telif choice \\== \"5\":  \n",
    "         \tdebugger \\= AgentDebugger()  \n",
    "         \tfilename \\= debugger.export\\_debug\\_data()  \n",
    "         \tprint(f\"✅ Data exported to: {filename}\")  \n",
    "     \telif choice \\== \"6\":  \n",
    "         \tprint(\"👋 Goodbye\\! Happy debugging\\!\")  \n",
    "         \tbreak  \n",
    "     \telse:  \n",
    "         \tprint(\"❌ Invalid option. Please select 1-6.\")\n",
    "\n",
    " if \\_\\_name\\_\\_ \\== \"\\_\\_main\\_\\_\":  \n",
    " \tinteractive\\_debugging\\_menu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02267db-6b5e-49ae-a589-a0f9375fe3af",
   "metadata": {},
   "source": [
    "**Step 6: Run Your Comprehensive Debug Session**\n",
    "\n",
    "1\\.   \t**Execute the full debugging suite**:\n",
    "\n",
    "o   Save all your files\n",
    "\n",
    "o   In the terminal, run: python run\\_comprehensive\\_debug.py\n",
    "\n",
    "o   Follow the interactive menu to run different types of tests\n",
    "\n",
    "2\\.   \t**What you should see during testing**:\n",
    "\n",
    "o   Detailed test progress with pass/fail indicators\n",
    "\n",
    "o   Quality scores for each response\n",
    "\n",
    "o   Error detection and classification\n",
    "\n",
    "o   Improvement suggestions based on performance\n",
    "\n",
    "o   Comprehensive reports with actionable insights\n",
    "\n",
    "**Understanding Your Debug Results**\n",
    "\n",
    "After running the debugging session, you'll have:\n",
    "\n",
    "·       **Edge Case Analysis**: Understanding how your agent handles unusual or challenging inputs\n",
    "\n",
    "·       **Validation Results**: Concrete accuracy measurements against known correct answers\n",
    "\n",
    "·       **Quality Metrics**: Scores indicating reasoning depth and coherence\n",
    "\n",
    "·       **Error Patterns**: Common types of mistakes your agent makes\n",
    "\n",
    "·       **Improvement Roadmap**: Specific suggestions for enhancing performance\n",
    "\n",
    "**Best Practices for Ongoing Debugging**\n",
    "\n",
    "1\\.   \t**Regular Testing Schedule**: Run comprehensive debugging sessions weekly\n",
    "\n",
    "2\\.   \t**Track Improvements**: Monitor trends over time to ensure progress\n",
    "\n",
    "3\\.   \t**Document Changes**: Record what modifications you make and their effects\n",
    "\n",
    "4\\.   \t**Version Control**: Keep backups of configurations that work well\n",
    "\n",
    "5\\.   \t**Incremental Improvements**: Make small changes and test their impact\n",
    "\n",
    "Create a feedback loop where you regularly review debugging logs, update your prompts and agent configurations based on learnings, and retest with both old and new test cases. This iterative process transforms debugging from a reactive chore into a proactive improvement strategy that makes your reasoning agent increasingly reliable and trustworthy over time.\n",
    "\n",
    "**Your Debugging System is Complete**\n",
    "\n",
    "Congratulations\\! You now have a professional-grade debugging and validation system that provides:\n",
    "\n",
    "·       ✅ **Comprehensive Logging**: Detailed records of all agent interactions\n",
    "\n",
    "·       ✅ **Edge Case Testing**: Systematic evaluation of challenging scenarios\n",
    "\n",
    "·       ✅ **Accuracy Validation**: Verification against known correct answers\n",
    "\n",
    "·       ✅ **Quality Assessment**: Measurement of reasoning depth and coherence\n",
    "\n",
    "·       ✅ **Error Detection**: Identification of common reasoning problems\n",
    "\n",
    "·       ✅ **Improvement Tracking**: Historical analysis of performance trends\n",
    "\n",
    "·       ✅ **Actionable Insights**: Specific suggestions for enhancement\n",
    "\n",
    "This systematic approach ensures your DeepSeek agent maintains high performance and continues improving over time. In the next chapter, you'll learn how to chain multiple agents together for even more sophisticated AI workflows\\!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
